searchState.loadedDescShard("polars", 0, "Polars: <em>DataFrames in Rust</em>\nPolars crate version\nThe typed heart of every Series column.\nData types supported by Polars.\nEnable the global string cache.\nDataFrame module.\nFunctions\nType agnostic columnar data structure.\nTesting utilities.\nCheck whether the global string cache is enabled.\nChunkedArray\nImplementations of arithmetic operations on ChunkedArrays.\nImplementations of the ChunkCast Trait.\nMethods for collecting into a ChunkedArray.\nReturns the argument unchanged.\nImplementations of upstream traits for <code>ChunkedArray&lt;T&gt;</code>\nCalls <code>U::from(self)</code>.\nTraits for miscellaneous operations on ChunkedArray\nTraits and utilities for temporal data.\nAppends a null slot into the builder\nAppends a null slot into the builder\nAppends a value of type <code>T</code> into the builder\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nCreate a new ChunkedArray from an iterator.\nCreate a new ChunkedArray from an iterator.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCreate a new BinViewChunkedBuilder\nOverflow is replaced with null\nAllows wrapping overflow\nRaises on overflow\nReturns the argument unchanged.\nCalls <code>U::from(self)</code>.\nAll masked out values have their nulls propagated.\nRecursive version of <code>CAN_FAST_EXPLODE_LIST</code>.\nAn interior mutable version of <code>StatisticsFlags</code>\nGet a flags value with all known bits set.\nThe bitwise and (<code>&amp;</code>) of the bits in two flags values.\nThe bitwise and (<code>&amp;</code>) of the bits in two flags values.\nThe bitwise or (<code>|</code>) of the bits in two flags values.\nThe bitwise or (<code>|</code>) of the bits in two flags values.\nGet the underlying bits value.\nThe bitwise exclusive-or (<code>^</code>) of the bits in two flags …\nThe bitwise exclusive-or (<code>^</code>) of the bits in two flags …\nThe bitwise negation (<code>!</code>) of the bits in a flags value, …\nWhether all set bits in a source flags value are also set …\nThe intersection of a source flags value with the …\nGet a flags value with all bits unset.\nThe bitwise or (<code>|</code>) of the bits in each flags value.\nReturns the argument unchanged.\nReturns the argument unchanged.\nConvert from a bits value.\nConvert from a bits value exactly.\nConvert from a bits value, unsetting any unknown bits.\nThe bitwise or (<code>|</code>) of the bits in each flags value.\nGet a flags value with the bits of a flag with the given …\nThe bitwise or (<code>|</code>) of the bits in two flags values.\nThe bitwise and (<code>&amp;</code>) of the bits in two flags values.\nWhether any set bits in a source flags value are also set …\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nWhether all known bits in this flags value are set.\nWhether all bits in this flags value are unset.\nYield a set of contained flags values.\nYield a set of contained named flags values.\nThe bitwise negation (<code>!</code>) of the bits in a flags value, …\nThe intersection of a source flags value with the …\nCall <code>insert</code> when <code>value</code> is <code>true</code> or <code>remove</code> when <code>value</code> is …\nThe intersection of a source flags value with the …\nThe intersection of a source flags value with the …\nThe bitwise exclusive-or (<code>^</code>) of the bits in two flags …\nThe bitwise exclusive-or (<code>^</code>) of the bits in two flags …\nThe bitwise or (<code>|</code>) of the bits in two flags values.\nThe no null iterator for a <code>BooleanArray</code>\nA <code>PolarsIterator</code> is an iterator over a <code>ChunkedArray</code> which …\nWrapper struct to convert an iterator of type <code>T</code> into one …\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\ncreate a new iterator\ncreate a new iterator\nValues need to implement this so that they can be stored …\nTrimmed down object safe polars object\nReturns the argument unchanged.\nCalls <code>U::from(self)</code>.\nCheck validity\nCheck validity\nReturns an iterator of <code>Option&lt;&amp;T&gt;</code> over every element of …\nThis is a heap allocated utility that can be used to …\nControl whether extension types may be created.\nSets the validity of this array.\nThis should be used as type information. Consider this a …\nGet a value at a certain index location\nGet a value at a certain index location\nReturns this array with a new validity.\nAppends a null slot into the builder\nAppends a value of type <code>T</code> into the builder\nReturns the argument unchanged.\nSafety\nCalls <code>U::from(self)</code>.\nThis trait can be registered, after which that global …\nTakes a <code>name</code> and <code>capacity</code> and constructs a new builder.\nAppend a <code>null</code> value.\nAppend a <code>T</code> of <code>ObjectChunked&lt;T&gt;</code> made generic via the <code>Any</code> …\nA function that creates an object builder\nReturns the argument unchanged.\nSafety\nCalls <code>U::from(self)</code>.\nTake the current state and materialize as a <code>Series</code> the …\nprevious value in array\nAggregation operations.\nFastest way to do elementwise operations on a …\nApply kernels on the arrow array chunks in a ChunkedArray.\nCast <code>ChunkedArray&lt;T&gt;</code> to <code>ChunkedArray&lt;N&gt;</code>\nCompare <code>Series</code> and <code>ChunkedArray</code>’s and get a <code>boolean</code> mask …\nCompare <code>Series</code> and <code>ChunkedArray</code>’s using inequality …\nCreate a new ChunkedArray filled with values at that index.\nExplode/flatten a List or String Series\nReplace None values with a value\nFilter values by a boolean mask.\nFill a ChunkedArray with one value.\nUtility methods for dealing with nested chunked arrays.\nQuantile and median aggregation.\nReverse a <code>ChunkedArray&lt;T&gt;</code>\nThis differs from ChunkWindowCustom and ChunkWindow by not …\nCreate a <code>ChunkedArray</code> with new values by index or by …\nShift the values of a <code>ChunkedArray</code> by a number of periods.\nSort operations on <code>ChunkedArray</code>.\nGet unique values in a <code>ChunkedArray</code>\nVariance and standard deviation aggregation.\nCombine two <code>ChunkedArray</code> based on some predicate.\nnext value in array\nMask the first unique values as <code>true</code>\nMask the last unique values as <code>true</code>\nmaximum value in array\nmean value of array\nminimal value in array\nNo value.\nreplace with the value one\nUtility trait to slice concrete arrow arrays whilst …\nSome value of type <code>T</code>.\nSort options for multi-series sorting.\nOptions for single series sorting.\nreplace with the value zero\nMeant for internal use. In very rare conditions this can …\nApply a closure elementwise including null values.\nApply kernel and return result as a new ChunkedArray.\nApply a kernel that outputs an array of different type.\nApply a closure elementwise and write results to a mutable …\nApply a closure elementwise. This is fastest when the null …\nRetrieve the indexes needed to sort this array.\nGet first index of the unique values in a <code>ChunkedArray</code>. …\nDoes not check if the cast is a valid one and may …\nCast a <code>ChunkedArray</code> to <code>DataType</code>\nIf true sort in descending order. Default <code>false</code>.\nOrder of the columns. Default all `false``.\nCheck for equality.\nCheck for equality where <code>None == None</code>.\nReplace None values with a give value <code>T</code>.\nFilter values in the ChunkedArray with a boolean mask.\nFind the indices of the values where the validity …\nCreate a ChunkedArray with a single value.\nGet a single value. Beware this is slow.\nGet a single value. Beware this is slow. If you need to …\nGreater than comparison.\nGreater than or equal comparison.\nLimit a sort output, this is for optimization purposes and …\nLimit a sort output, this is for optimization purposes and …\nLess than comparison.\nLess than or equal comparison\nIf true maintain the order of equal elements. Default <code>false</code>…\nWhether maintain the order of equal elements. Default <code>false</code>…\nIf true sort in multiple threads. Default <code>true</code>.\nWhether sort in multiple threads. Default <code>true</code>.\nCreate a new ChunkedArray filled with values at that index.\nCheck for inequality.\nCheck for inequality where <code>None == None</code>.\nWhether place null values last. Default <code>false</code>.\nWhether place null values last. Default <code>false</code>.\nPropagate nulls of nested datatype to all levels of …\nReturn a reversed version of this array.\nSet the values at indexes <code>idx</code> to some optional value …\nSet the values at indexes <code>idx</code> by applying a closure to …\nSet the values where the mask evaluates to <code>true</code> to some …\nShift the values by a given period and fill the parts that …\nSlices this <code>Array</code>.\nSlices the <code>Array</code>.\nReturned a sorted <code>ChunkedArray</code>.\nGather values from ChunkedArray by index.\nGather values from ChunkedArray by index.\nTrim all lists of unused start and end elements …\nGet unique values of a ChunkedArray\nCreate a new ChunkedArray with values from self where the …\nApplies a kernel that produces <code>Array</code> types.\nApply elementwise binary function which produces string, …\nApplies a kernel that produces <code>Array</code> types.\nApplies a kernel that produces <code>Array</code> types.\nApplies a kernel that produces <code>Array</code> types.\nApplies a kernel that produces <code>ArrayRef</code> of the same type.\nApplies a kernel that produces <code>Array</code> types.\nApplies a kernel that produces <code>ArrayRef</code> of the same type.\nApplies a kernel that produces <code>Array</code> types.\nApplies a kernel that produces <code>Array</code> types.\nApplies a kernel that produces <code>Array</code> types.\nApplies a kernel that produces <code>Array</code> types.\nSafety\nSafety\nGet the <code>RowEncodingContext</code> for a certain <code>DataType</code>.\nSearch through a series of chunks for the first position …\nUtility trait to slice concrete arrow arrays whilst …\nSlices this <code>Array</code>.\nSlices the <code>Array</code>.\nReturn the indices of the bottom k elements.\nUtility trait to slice concrete arrow arrays whilst …\nSort options for multi-series sorting.\nOptions for single series sorting.\nIf true sort in descending order. Default <code>false</code>.\nOrder of the columns. Default all `false``.\nLimit a sort output, this is for optimization purposes and …\nLimit a sort output, this is for optimization purposes and …\nIf true maintain the order of equal elements. Default <code>false</code>…\nWhether maintain the order of equal elements. Default <code>false</code>…\nIf true sort in multiple threads. Default <code>true</code>.\nWhether sort in multiple threads. Default <code>true</code>.\nWhether place null values last. Default <code>false</code>.\nWhether place null values last. Default <code>false</code>.\nSlices this <code>Array</code>.\nSlices the <code>Array</code>.\nA nested list with a fixed size in each row\nThe set of supported logical types in this crate.\nThe time units defined in Arrow.\nOpaque binary data of variable length whose offsets are …\nA binary type that inlines small values and can intern …\nA binary true or false.\n<code>true</code> and <code>false</code>.\nA 32-bit date representing the elapsed time since UNIX …\nA 32-bit date representing the elapsed time since UNIX …\nAn <code>i32</code> representing the elapsed time since UNIX epoch …\nAn <code>i64</code> representing the elapsed time since UNIX epoch …\nA 64-bit date representing the elapsed time since UNIX …\nA 64-bit date representing the elapsed time since UNIX …\nA 64-bit date representing the elapsed time since UNIX …\nA 128-bit fixed point decimal number with a scale.\nDecimal value with precision and scale precision is the …\nFixed point decimal type optional precision and …\nDecimal backed by 256 bits\nA dictionary encoded array (<code>key_type</code>, <code>value_type</code>), where …\nA 64-bit integer representing difference between …\nMeasure of elapsed time. This elapsed time is a physical …\n64-bit integer representing difference between times in …\nExtension type.\nCharacterizes the name and the <code>DataType</code> of a column.\nOpaque binary data of fixed size. Enum parameter specifies …\nA list of some logical data type with a fixed number of …\nAn 16-bit float\nA 32-bit floating point number.\nA <code>f32</code>\nA 64-bit floating point number.\nA <code>f64</code>\nHashmap: maps the indexes from the global …\nA 128-bit integer number.\nAn <code>i128</code>\nA 16-bit integer number.\nAn <code>i16</code>\nA 32-bit integer number.\nAn <code>i32</code>\nA 64-bit integer number.\nAn <code>i64</code>\nAn 8-bit integer number.\nAn <code>i8</code>\nA “calendar” interval modeling elapsed time that takes …\nOpaque binary data of variable length whose offsets are …\nA list of some logical data type whose offsets are …\nA variable-length UTF-8 encoded string whose offsets are …\nNested type, contains arrays that are filled with one of …\nA list of some logical data type whose offsets are …\nA nested list with a variable size in each row\nUtf8Array: caches the string values and a hash of all …\nMaps a logical type to a chunked array implementation of …\nA nested type that is represented as\nTime in microseconds.\nTime in milliseconds.\nTime in nanoseconds.\nNull type\nCan be used to fmt and implements Any, so can be …\nA generic type that can be used in a <code>Series</code> &amp;’static str …\nThis hashmap uses an IdHasher\nString type that inlines small strings.\nSafety\nA dimension in a reshape.\nTime in seconds.\nA UTF8 encoded string type.\nString data\nAn UTF8 encoded string type.\nA nested <code>ArrowDataType</code> with a given number of <code>Field</code>s.\nA 64-bit time representing the elapsed time since midnight …\nA 64-bit time representing the elapsed time since midnight …\nA 32-bit time representing the elapsed time since midnight …\nA 64-bit time representing the elapsed time since midnight …\nA <code>i64</code> representing a timestamp measured in <code>TimeUnit</code> with …\nAn unsigned 16-bit integer number.\nAn <code>u16</code>\nAn unsigned 32-bit integer number.\nAn <code>u32</code>\nAn unsigned 64-bit integer number.\nAn <code>u64</code>\nAn unsigned 8-bit integer number.\nAn <code>u8</code>\nA nested datatype that can represent slots of differing …\nA type unknown to Arrow.\nA variable-length UTF-8 encoded string whose offsets are …\nA string type that inlines small values and can intern …\nGet data type of <code>ChunkedArray</code>.\nReturns the DataType variant associated with this …\nSafety\nHashmap: maps the indexes from the global …\nUtf8Array: caches the string values and a hash of all …\nHashmap: maps the indexes from the global …\nUtf8Array: caches the string values and a hash of all …\nEnable the global string cache as long as the object is …\nDisable and clear the global string cache.\nEnable the global string cache.\nCheck whether the global string cache is enabled.\nReturns the argument unchanged.\nCalls <code>U::from(self)</code>.\nConvert fixed offset to Etc/GMT one from time zone database\nParse a time zone string to <code>chrono_tz::Tz</code>\nPolars Eager cookbook\nPolars Lazy cookbook\nContains the error value\nContains the success value\nConstant that help with creating error messages dependent …\nReturns the argument unchanged.\nReturns the argument unchanged.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nSet the function that will be called by the <code>polars_warn!</code> …\nPython hooks SIGINT to instead generate a …\nRuns the passed function, catching any KeyboardInterrupts …\nReturns the argument unchanged.\nCalls <code>U::from(self)</code>.\nChecks if the keyboard interrupt flag is set, and if yes …\nKeep any of the unique rows This allows more optimizations\nA contiguous growable collection of <code>Series</code> that have the …\nKeep the first unique row.\nKeep the last unique row.\nKeep None of the unique rows.\nSplit DataFrame into chunks in preparation for writing. …\nReturns the argument unchanged.\nReturns the argument unchanged.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nExtends this builder with the contents of the given …\nReturns the argument unchanged.\nExtends this builder with the contents of the given …\nCalls <code>U::from(self)</code>.\nExtends this builder with the contents of the given …\nExtends this builder with the contents of the given …\nExtends this builder with the contents of the given …\nExtends this builder with the contents of the given …\nA column within a <code>DataFrame</code>.\nConvert <code>Self</code> into a <code>Column</code>\nA <code>Column</code> that consists of a repeated <code>Scalar</code>\nGet the <code>ScalarColumn</code> as <code>Series</code>\nTake the <code>ScalarColumn</code> as a series with a <code>n</code> values.\nTake the <code>ScalarColumn</code> as a series with a single value.\nSafety\nReturns the argument unchanged.\nCreate a new <code>ScalarColumn</code> from a <code>length=1</code> Series and …\nCalls <code>U::from(self)</code>.\nGet the <code>ScalarColumn</code> as <code>Series</code> if it was already …\nResize the <code>ScalarColumn</code> to new <code>length</code>.\nTake the <code>ScalarColumn</code> and materialize as a <code>Series</code> if not …\nMaterialize the <code>ScalarColumn</code> into a <code>Series</code>.\nCreate a new <code>ScalarColumn</code> from a <code>length=1</code> Series and …\nArguments for <code>LazyFrame::unpivot</code> function\nReturned by a group_by operation on a DataFrame. This …\nIndexes of the groups, the first index is stored …\nEvery group is indicated by an array where the\nUsed to create the tuples for a group_by operation.\nSlice is always sorted in ascending order.\nHelper that combines the groups into a parallel iterator …\nSame helper as <code>_agg_helper_idx</code> but for aggregations that …\nSafety\nAn <code>AnyValueBuffer</code> that should be used when we trust the …\nSafety\nWill add the <code>AnyValue</code> into <code>Self</code> and unpack as the physical …\nCoerces a slice of datatypes into a single supertype.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nClear <code>self</code> and give <code>capacity</code>, returning the old contents …\nInfer schema from rows and set the first no null type as …\nInfer the schema of rows by determining the supertype of …\nInfer the schema data types of rows by determining the …\nConcat <code>DataFrame</code>s diagonally. Concat diagonally thereby …\nConcat <code>DataFrame</code>s horizontally. Concat horizontally and …\nMultiple values that are used for all columns\nA single value that’s used for all columns\nAllowedOptimizations\nQuote every field. Always.\nKeep any of the unique rows This allows more optimizations\nA thread-safe reference-counting pointer. ‘Arc’ stands …\nArgmin/ Argmax\nA nested list with a fixed size in each row\nSpecialized expressions for <code>Series</code> of <code>DataType::Array</code>.\nThe set of supported logical types in this crate.\nRepresents Arrow’s metadata of a “column”.\nAn ordered sequence of <code>Field</code>s\nThe time units defined in Arrow.\nAutomatically determine over which unit to parallelize …\nselects the last row in the right DataFrame whose ‘on’ …\nprevious value in array\nOpaque binary data of variable length whose offsets are …\nA binary type that inlines small values and can intern …\nA binary true or false.\n<code>true</code> and <code>false</code>.\nA valid Brotli compression level.\nCheck if operations are order dependent and unset …\nCluster sequential <code>with_columns</code> calls to independent calls.\nCollapse slower joins with filters into faster joins.\nRun common-subexpression-elimination. This elides …\nRun common-subplan-elimination. This elides duplicate …\nCache the input at this point in the LP\nUsed by scans.\nSpecialized expressions for Categorical dtypes.\nUtility struct for the <code>when-then-otherwise</code> expression.\nUtility struct for the <code>when-then-otherwise</code> expression.\nAggregation operations.\nAggregations that return <code>Series</code> of unit length. Those can …\nFastest way to do elementwise operations on a …\nApply kernels on the arrow array chunks in a ChunkedArray.\nCast <code>ChunkedArray&lt;T&gt;</code> to <code>ChunkedArray&lt;N&gt;</code>\nCompare <code>Series</code> and <code>ChunkedArray</code>’s and get a <code>boolean</code> mask …\nCompare <code>Series</code> and <code>ChunkedArray</code>’s using inequality …\nCreate a new ChunkedArray filled with values at that index.\nExplode/flatten a List or String Series\nReplace None values with a value\nFilter values by a boolean mask.\nFill a ChunkedArray with one value.\nUtility methods for dealing with nested chunked arrays.\nQuantile and median aggregation.\nReverse a <code>ChunkedArray&lt;T&gt;</code>\nThis differs from ChunkWindowCustom and ChunkWindow by not …\nCreate a <code>ChunkedArray</code> with new values by index or by …\nShift the values of a <code>ChunkedArray</code> by a number of periods.\nSort operations on <code>ChunkedArray</code>.\nGet unique values in a <code>ChunkedArray</code>\nVariance and standard deviation aggregation.\nCombine two <code>ChunkedArray</code> based on some predicate.\nChunkedArray\nA column within a <code>DataFrame</code>.\nA wrapper trait for any binary closure …\nParallelize over the columns\nA wrapper trait for any closure …\nCreate a new DataFrame by reading a csv file.\nWrite a DataFrame to csv.\nOptions for writing CSV files.\nA contiguous growable collection of <code>Series</code> that have the …\nIn memory DataFrame\nA 32-bit date representing the elapsed time since UNIX …\nA 32-bit date representing the elapsed time since UNIX …\nAn <code>i32</code> representing the elapsed time since UNIX epoch …\nAn <code>i64</code> representing the elapsed time since UNIX epoch …\nA 64-bit date representing the elapsed time since UNIX …\nA 64-bit date representing the elapsed time since UNIX …\nArguments used by <code>datetime</code> in order to produce an <code>Expr</code> of …\nA 64-bit date representing the elapsed time since UNIX …\nA 128-bit fixed point decimal number with a scale.\nDecimal value with precision and scale precision is the …\nFixed point decimal type optional precision and …\nDecimal backed by 256 bits\nA dictionary encoded array (<code>key_type</code>, <code>value_type</code>), where …\nRemove duplicates from the table\nA 64-bit integer representing difference between …\nMeasure of elapsed time. This elapsed time is a physical …\n64-bit integer representing difference between times in …\nArguments used by <code>duration</code> in order to produce an <code>Expr</code> of …\nA dynamically inferred literal value. This needs to be …\nRust function to dynamically compute key value metadata.\nRun every node eagerly. This turns off multi-node …\nConfiguration variant that defaults to raising on mismatch.\nContains the error value\nCan be used in a select statement to exclude a column from …\nExplode the aggregated list and just do a hstack instead …\nExpressions that can be used in various contexts.\nSpecialized expressions for modifying the name of existing …\nThis allows expressions to access other tables\nExtension type.\nReplace simple projections with a faster inlined …\nCharacterizes the name and the <code>DataType</code> of a column.\nMetadata for a Parquet file.\nFilter on a boolean mask\nKeep the first unique row.\nOpaque binary data of fixed size. Enum parameter specifies …\nA list of some logical data type with a fixed number of …\nAn 16-bit float\nA 32-bit floating point number.\nA <code>f32</code>\nA 64-bit floating point number.\nA <code>f64</code>\nselects the first row in the right DataFrame whose …\nnext value in array\nHashmap: maps the indexes from the global …\nReturned by a group_by operation on a DataFrame. This …\nGroupby aggregation\nIndexes of the groups, the first index is stored …\nEvery group is indicated by an array where the\nMap the group values to the position\nA valid Gzip compression level.\nHorizontal concatenation of multiple plans\nAdding columns to the table without a Join\nPolars IdxSize type, dependent on bigidx feature\nInserts full-NULL columns for the missing ones.\nA 128-bit integer number.\nAn <code>i128</code>\nA 16-bit integer number.\nAn <code>i16</code>\nA 32-bit integer number.\nAn <code>i32</code>\nA 64-bit integer number.\nAn <code>i64</code>\nAn 8-bit integer number.\nAn <code>i8</code>\nA “calendar” interval modeling elapsed time that takes …\nConvert <code>Self</code> into a <code>Column</code>\nUsed to create the tuples for a group_by operation.\nUsed to convert a <code>ChunkedArray</code>, <code>&amp;dyn SeriesTrait</code> and <code>Series</code>\nConvenience for <code>x.into_iter().map(Into::into).collect()</code> …\nCompression codec\nRead Arrows IPC format into a DataFrame\nAn Arrow IPC reader implemented on top of …\nRead Arrows Stream IPC format into a DataFrame\nWrite a DataFrame to Arrow’s Streaming IPC format\nWrite a DataFrame to Arrow’s IPC format\nMask the first unique values as <code>true</code>\nMask the last unique values as <code>true</code>\nJoin the groups as ‘List&lt;group_dtype&gt;’ to the row …\nJoin operation\nA single JSON array containing each DataFrame row as an …\nThe format to use to write the DataFrame to JSON: <code>Json</code> (a …\nEach DataFrame row is serialized as a JSON object on a …\nReads JSON in one of the formats in <code>JsonFormat</code> into a …\nWrites a DataFrame to JSON.\nSet root name as Alias\nKey/value pairs that can be attached to a Parquet file as …\nLZ4 (framed)\nOpaque binary data of variable length whose offsets are …\nA list of some logical data type whose offsets are …\nA variable-length UTF-8 encoded string whose offsets are …\nKeep the last unique row.\nReads LazyFrame from a filesystem or a cloud storage. …\nLazy abstraction over an eager <code>DataFrame</code>.\nUtility struct for lazy group_by operation.\nA value of type <code>L</code>.\nNested type, contains arrays that are filled with one of …\nA list of some logical data type whose offsets are …\nA nested list with a variable size in each row\nList / Array\nSpecialized expressions for <code>Series</code> of <code>DataType::List</code>.\nUtf8Array: caches the string values and a hash of all …\nMaps a logical type to a chunked array implementation of …\nUtf8 encoding and unknown bytes are replaced with �.\nNo unique checks\nCheck if join keys are unique in right dataset.\nA nested type that is represented as\nA (User Defined) Function\nMatch / Evolve into a schema\nmaximum value in array\nmean value of array\nTime in microseconds.\nTime in milliseconds.\nminimal value in array\nonly useful if periods are weekly\nA string that indicates the start of a comment line. This …\nTuples that map column names to null value of that column\nTime in nanoseconds.\nselects the right in the right DataFrame whose ‘on’ …\nQuote fields only when necessary.\nNever quote any fields, even if it would produce invalid …\nJust a wrapper structure which is useful for certain impl …\nQuote non-numeric fields.\nNo value.\nDon’t parallelize\nFlat datatypes\nKeep None of the unique rows.\nTake the nth column in the <code>DataFrame</code>\nThe literal Null\nNull type\nCan be used to fmt and implements Any, so can be …\nA generic type that can be used in a <code>Series</code> &amp;’static str …\nContains the success value\nreplace with the value one\nCheck if join keys are unique in left dataset.\nCheck if join keys are unique in both left and right …\nAllowed optimizations.\nExplode the aggregated list and just do a hstack instead …\nApply predicates/filters as early as possible.\nOnly read columns that are used later in the query.\nThe compression strategy to use for writing Parquet files.\nContext that can be used to construct custom file-level …\nRead Apache parquet format into a DataFrame.\nParquet statistics for a nesting level\nWrite a DataFrame to Parquet format.\nThis hashmap uses an IdHasher\nString type that inlines small strings.\nSafety\nA <code>PolarsIterator</code> is an iterator over a <code>ChunkedArray</code> which …\nValues need to implement this so that they can be stored …\nFirst evaluates the pushed-down predicates in parallel and …\nQuote style indicating when to insert quotes around a …\nTry to estimate the number of rows so that joins can …\nError if there are extra columns outside the target schema.\nA dimension in a reshape.\nA value of type <code>R</code>.\nParallelize over the row groups\nRun many expression optimization rules until fixed point.\nPushdown slices/limits.\nRun nodes that are capably of doing so on the streaming …\nA single source to scan from\nAn iterator for <code>ScanSources</code>\nA reference to a single item in <code>ScanSources</code>\nSet of sources to scan from\nTime in seconds.\nPolars’ <code>select</code> operation, this can mean projection, but …\nExpressions in this node should only be expanding e.g. …\nOptions to serialize logical types to CSV.\nSeries\nA single byte character that indicates the start of a …\nOptions that apply to all sinks.\nSlice the table\nSlice is always sorted in ascending order.\nUtility trait to slice concrete arrow arrays whilst …\nSome value of type <code>T</code>.\nSort the table\nSort options for multi-series sorting.\nOptions for single series sorting.\nWrapper type that has special equality properties …\nStatic key value metadata.\nThe statistics to write\nA UTF8 encoded string type.\nString data\nEnable the global string cache as long as the object is …\nAn UTF8 encoded string type.\nA nested <code>ArrowDataType</code> with a given number of <code>Field</code>s.\nA <code>StructArray</code> is a nested <code>Array</code> with an optional validity …\nSpecialized expressions for Struct dtypes.\nDo type checking of the IR.\nRun many type coercion optimization rules until fixed …\nGather by <code>ChunkId</code>\nA ternary operation if true then “foo” else “bar”\nUtility struct for the <code>when-then-otherwise</code> expression.\nA 64-bit time representing the elapsed time since midnight …\nA 64-bit time representing the elapsed time since midnight …\nA 32-bit time representing the elapsed time since midnight …\nA 64-bit time representing the elapsed time since midnight …\nA <code>i64</code> representing a timestamp measured in <code>TimeUnit</code> with …\nAn unsigned 16-bit integer number.\nAn <code>u16</code>\nAn unsigned 32-bit integer number.\nAn <code>u32</code>\nAn unsigned 64-bit integer number.\nAn <code>u64</code>\nAn unsigned 8-bit integer number.\nAn <code>u8</code>\nScan arguments shared across different scan types.\nVertical concatenation\nA nested datatype that can represent slots of differing …\nA type unknown to Arrow.\nArguments for <code>LazyFrame::unpivot</code> function\nRepresents a user-defined function\nUtf8 encoding.\nA variable-length UTF-8 encoded string whose offsets are …\nA string type that inlines small values and can intern …\nUtility struct for the <code>when-then-otherwise</code> expression.\nRepresents a window in time\nPolars flavored window functions.\nZSTD\nreplace with the value zero\nA valid Zstandard compression level.\nTrue if all categories are represented in this array. When …\nSafety\nSame as <code>filter</code> but does not parallelize.\nSafety\nThis will not panic even in debug mode - there are some …\nThe schema names must match the column names of this …\nMeant for internal use. In very rare conditions this can …\nReturns the sum of the array as an f64.\nReturns the sum of the array as an f64.\nSafety\nSafety\nCreate a <code>DataFrame</code> that has fields for all the known …\nSafety\nCreate a new Series without checking if the inner dtype of …\nRun every node eagerly. This turns off multi-node …\nSet <code>FAST_UNIQUE</code> metadata\nConvert numerical values to their absolute value.\nConvert all values to their absolute/positive value.\nGroup by and aggregate.\nSafety\nGet the group indexes of the group by operation.\nSafety\nSafety\nAggregate the groups of the group_by operation into lists.\nSafety\nSafety\nSafety\nSafety\nSafety\nSafety\nSafety\nSafety\nSafety\nSafety\nRename Column.\nEnsure all the chunks in the <code>DataFrame</code> are aligned.\nSelects all columns. Shorthand for <code>col(&quot;*&quot;)</code>.\nReturns whether all values in the column are <code>true</code>.\nReturns whether all values in the array are <code>true</code>.\nGet a flags value with all known bits set.\nGet a flags value with all known bits set.\nCreate a new column with the bitwise-and of the elements …\nReturns whether all values in the column are <code>true</code>.\nReturns a reference to the underlying allocator.\nAllow equal matches\nAllow parallel table evaluation.\nSpecify if the scan provider should allow predicate …\nSpecify if the scan provider should allow projection …\nSpecify if the scan provider should allow slice pushdowns.\nThis is an iterator over a <code>ListChunked</code> that saves …\nThis is an iterator over a <code>ArrayChunked</code> that save …\nThis is an iterator over a <code>ArrayChunked</code> that save …\nSee <code>amortized_iter</code>.\nBitwise “and” operation.\nGet the bitwise AND of the Series as a new Series of …\nGet the bitwise AND of the Series as a new Series of …\nLeft anti join this query with another lazy query.\nReturns whether any of the values in the column are <code>true</code>.\nReturns whether any of the values in the column are <code>true</code>.\nCreate a new column with the bitwise-or of the elements in …\nReturns whether any of the values in the column are <code>true</code>.\nAppend expressions. This is done by adding the chunks of …\nAppend in place. This is done by adding the chunks of <code>other</code>…\nAppend in place. This is done by adding the chunks of <code>other</code>…\nAppends from an iterator over values\nAppends a null slot into the builder\nAppends a null slot into the builder\nAppend in place. This is done by adding the chunks of <code>other</code>…\nAppend in place. This is done by adding the chunks of <code>other</code>…\nAppends a value of type <code>T</code> into the builder\nAppends a value of type <code>T</code> into the builder\nAppends from an iterator over values\nApply a closure elementwise including null values.\nApply a function over the groups as a new DataFrame.\nApply a function/closure over the groups. This should only …\nApply a closure over the groups as a new <code>DataFrame</code>.\nApply a closure to a column. This is the recommended way …\nApply a closure <code>F</code> elementwise.\nApply a closure <code>F</code> elementwise.\nApply a closure <code>F</code> elementwise.\nApply a closure <code>F</code> to each array.\nApply a closure to a column at index <code>idx</code>. This is the …\nLike <code>map_binary</code>, but used in a group_by-aggregation …\nCast a numeric array to another numeric data type and …\nApply kernel and return result as a new ChunkedArray.\nApply a kernel that outputs an array of different type.\nApply a function/closure over the groups with many …\nApply a function/closure over the groups of multiple …\nApplies a function only to the non-null elements, …\nIgnore the list indices and apply <code>func</code> to the inner type …\nIgnore the list indices and apply <code>func</code> to the inner type …\nApply a closure elementwise and write results to a mutable …\nApply a closure elementwise. This is fastest when the null …\nApply a closure <code>F</code> elementwise.\nSafety\nGet the approximate count of unique values.\nGenerate a range of integers.\nArcs this array into a <code>std::sync::Arc&lt;dyn Array&gt;</code>.\nGet the index of the maximal value\nReturn the index of the maximum value of every sublist\nGet the index value that has the maximum value.\nGet the index of the minimal value\nReturn the index of the minimal value of every sublist\nGet the index value that has the minimum value.\nRetrieve the indexes needed to sort this array.\nGet the index values that would sort this expression.\nRetrieve the indexes needed for a sort.\nRetrieve the indexes needed for a sort.\nRetrieve the indexes needed to sort this array.\nFind the indexes that would sort these series in order of …\nRetrieve the indexes need to sort this and the other …\nRetrieve the indexes need to sort this and the other …\nPanics\nPanics\nPanics\nGet first index of the unique values in a <code>ChunkedArray</code>. …\nGet the first index of unique values of this expression.\nGet first indexes of unique values.\nGet first indexes of unique values.\nGet the indices where <code>condition</code> evaluates <code>true</code>.\nGet the <code>array::ArrayNameSpace</code>.\nUnpack to <code>ChunkedArray</code> of dtype <code>DataType::Array</code>\nGet the inner data type of a multidimensional array.\nReturns a reference to the Arrow ArrayRef\nGet arrow schema of the Ipc Stream File, this is faster …\nGet a hold of the <code>ChunkedArray</code>, <code>Logical</code> or <code>NullChunked</code> as …\nGet a hold of the <code>ChunkedArray</code>, <code>Logical</code> or <code>NullChunked</code> as …\nParsing string values and return a <code>DateChunked</code>\nParsing string values and return a <code>DateChunked</code>\nParsing string values and return a <code>DateChunked</code>\nParsing string values and return a <code>DateChunked</code> Different …\nParsing string values and return a <code>DateChunked</code> Different …\nParsing string values and return a <code>DateChunked</code> Different …\nParsing string values and return a <code>DatetimeChunked</code>.\nParsing string values and return a <code>DatetimeChunked</code>.\nParsing string values and return a <code>DatetimeChunked</code>.\nParsing string values and return a <code>DatetimeChunked</code> …\nParsing string values and return a <code>DatetimeChunked</code> …\nParsing string values and return a <code>DatetimeChunked</code> …\nPacks every element into a list.\nPacks every element into a list.\nGet a reference to a <code>Series</code> for this <code>Column</code>\nIf the memory repr of this Column is a scalar, a …\nTry cast the scan sources to <code>ScanSources::Paths</code>\nGet a hold of the <code>ChunkedArray</code> or <code>NullChunked</code> as an <code>Any</code> …\nProvides a raw pointer to the data.\nAggregate all the chunks in the DataFrame to a single …\nAggregate all the chunks in the DataFrame to a single …\nRechunk and return a pointer to the start of the Series. …\nRechunk and return a pointer to the start of the Series. …\nOnly implemented for numeric types\nTake several expressions and collect them into a …\nParsing string values and return a <code>TimeChunked</code>\nParsing string values and return a <code>TimeChunked</code>\nParsing string values and return a <code>TimeChunked</code>\nConverts to <code>Arc&lt;[T]&gt;</code>.\nConverts to <code>Arc&lt;T&gt;</code>.\nGet the scan source at specific address\nFind the mean of all the values in the column named <code>name</code>. …\nGet the <code>binary::BinaryNameSpace</code>\nUnpack to <code>ChunkedArray</code> of dtype <code>DataType::Binary</code>\nCompute <code>op(l, r)</code> (or equivalently <code>l op r</code>). <code>l</code> and <code>r</code> must …\nUnpack to <code>ChunkedArray</code> of dtype <code>DataType::Binary</code>\nThe bitwise and (<code>&amp;</code>) of the bits in two flags values.\nThe bitwise and (<code>&amp;</code>) of the bits in two flags values.\nThe bitwise and (<code>&amp;</code>) of the bits in two flags values.\nThe bitwise and (<code>&amp;</code>) of the bits in two flags values.\nThe bitwise or (<code>|</code>) of the bits in two flags values.\nThe bitwise or (<code>|</code>) of the bits in two flags values.\nThe bitwise or (<code>|</code>) of the bits in two flags values.\nThe bitwise or (<code>|</code>) of the bits in two flags values.\nGet the underlying bits value.\nGet the underlying bits value.\nThe bitwise exclusive-or (<code>^</code>) of the bits in two flags …\nThe bitwise exclusive-or (<code>^</code>) of the bits in two flags …\nThe bitwise exclusive-or (<code>^</code>) of the bits in two flags …\nThe bitwise exclusive-or (<code>^</code>) of the bits in two flags …\nUnpack to <code>ChunkedArray</code> of dtype <code>DataType::Boolean</code>\nBoxes this array into a <code>Box&lt;dyn Array&gt;</code>.\nCaches the result into a new LazyFrame.\nuse a cache of unique, converted dates to apply the …\ncreates a logical expression with a call of the UDF\nReturn whether the cast to <code>to</code> makes sense.\nCan the <code>AnyValue</code> exist as having <code>dtype</code> as its <code>DataType</code>.\nCancel the query at earliest convenience.\nCasts the column given by <code>Expr</code> to a different type.\nCast named frame columns, resulting in a new LazyFrame …\nCast a <code>ChunkedArray</code> to <code>DataType</code>\nCast a <code>ChunkedArray</code> to <code>DataType</code>\nCast expression to another data type.\nCast all frame columns to the given dtype, resulting in a …\nCast a numeric array to another numeric data type and …\nCast the leaf types of Lists/Arrays and keep the nesting.\nChange the underlying <code>TimeUnit</code>. And update the data …\nChange the underlying <code>TimeUnit</code>. And update the data …\nDoes not check if the cast is a valid one and may …\nSafety\nCast from physical to logical types without any checks on …\nCast a <code>ChunkedArray</code> to <code>DataType</code>\nCast expression to another data type.\nCast <code>Series</code> to another <code>DataType</code>.\nGet the <code>CategoricalNameSpace</code>.\nUnpack to <code>ChunkedArray</code> of dtype <code>DataType::Categorical</code>\nCompute the cube root of the given expression\nCeil underlying floating point array to the highest …\nCeil underlying floating point array to the highest …\nSet the labels at the center of the window.\nCalculate the millennium from the underlying NaiveDateTime …\nCalculate the millennium from the underlying NaiveDateTime …\nChecked integer division. Computes self / rhs, returning …\nGet the lengths of the underlying chunks\nReturns an iterator over the lengths of the chunks of the …\nSize of each written chunk.\nTraits and utilities for temporal data.\nUnderlying chunks.\nA reference to the chunks\nUnderlying chunks.\nA mutable reference to the chunks\nSafety\nRemove all the columns in the <code>DataFrame</code> but keep the <code>height</code>…\nSet values outside the given boundaries to the boundary …\nClip underlying values to a set boundary.\nSet values above the given maximum to the maximum value.\nClip underlying values to a set boundary.\nSet values below the given minimum to the minimum value.\nClip underlying values to a set boundary.\nMakes a clone of the <code>Arc</code> pointer.\nClone inner ChunkedArray and wrap in a new Arc\nWhich side windows should be closed.\nInterface with cloud storage through the object_store …\nCloudOptions used to list files.\nCloudOptions used to list files.\nCloudOptions used to list files.\nComparison for two <code>Arc</code>s.\nFolds the expressions from left to right keeping the first …\nWhether to coalesce join columns.\nSets the <code>Field</code> datatype.\nCreate a Column Expression based on a column name.\nExecute all the lazy operations and collect them into a …\nTurn the metadata into the key/value pairs to write to the …\nCollect all <code>LazyFrame</code> computations.\nGet a handle to the schema — a map from column names to …\nExecute all the lazy operations and collect them into a …\nSelect multiple columns by name.\nSelect a single column by name.\nExtend the columns without checking for name collisions or …\nReturns column order for <code>i</code>th column in this file. If …\nColumn (sort) order used for <code>min</code> and <code>max</code> values of each …\nSelected multiple columns by name.\nCompatibility level\nThe bitwise negation (<code>!</code>) of the bits in a flags value, …\nThe bitwise negation (<code>!</code>) of the bits in a flags value, …\nData page compression\nData page compression\nRedo a length and null_count compute\nCompute the schema. This requires conversion to <code>IR</code> and …\nConcat multiple <code>LazyFrame</code>s vertically.\nConcat with the values from a second StringChunked.\nHorizontally concatenate columns into a single array-type …\nRecommended concatenation of LazyFrames from many input …\nConcat LazyFrames diagonally. Calls <code>concat</code> internally.\nConcat LazyFrames horizontally.\nConcat lists entries.\nHorizontally concat string columns in linear time\nReturns the values of the array as a contiguous slice.\nCheck if the sub-array contains specific element\nCheck if binary contains given literal\nCheck if strings contain a regex pattern.\nCheck if the list array contain an element\nWhether all set bits in a source flags value are also set …\nWhether all set bits in a source flags value are also set …\nCheck (recursively) whether datatype contains an …\nCheck if strings contain a given literal\nCast null arrays to inner type and ensure that all offsets …\nReturn the number of non-null elements for each column.\nCount the values of the Series or Get counts of the group …\nAggregate grouped series and compute the number of values …\nCount all successive non-overlapping regex matches.\nCount all successive non-overlapping regex matches.\nRead the number of rows without parsing columns useful for …\nRead the number of rows without parsing columns\nRead the number of rows without parsing columns useful for …\nSafety\nString message for application that wrote this file.\nCreates the Cartesian product from both frames, preserving …\nCreates the Cartesian product from both frames, preserves …\nCumulatively count values from 0 to len.\nAccumulate over multiple columns horizontally / row wise.\nGet an array with the cumulative max computed at every …\nGet an array with the cumulative max computed at every …\nGet an array with the cumulative min computed at every …\nGet an array with the cumulative min computed at every …\nGet an array with the cumulative product computed at every …\nGet an array with the cumulative product computed at every …\nAccumulate over multiple columns horizontally / row wise.\nGet an array with the cumulative sum computed at every …\nGet an array with the cumulative sum computed at every …\nRun an expression over a sliding window that increases <code>1</code> …\nGet schema-level custom metadata of the Ipc file\nGet schema-level custom metadata of the Ipc Stream file\nif <code>None</code> will be 1024^2 bytes\nGet slices of the underlying arrow data. NOTE: null values …\nData types supported by Polars.\nUnpack to <code>ChunkedArray</code> of dtype <code>DataType::Date</code>\nUsed for <code>DataType::Date</code>.\nCreate a column of date ranges from a <code>start</code> and <code>stop</code> …\nConstruct a column of <code>Datetime</code> from the provided …\nUnpack to <code>ChunkedArray</code> of dtype <code>DataType::Datetime</code>\nAllow casting to change time units.\nUsed for <code>DataType::Datetime</code>.\nAllow datetime[ns] to be casted to any lower precision. …\nCreate a datetime range from a <code>start</code> and <code>stop</code> expression.\nCreate a column of datetime ranges from a <code>start</code> and <code>stop</code> …\nExtract day from underlying NaiveDate representation. …\nExtract day from underlying NaiveDate representation. …\nExtract day from underlying NaiveDateTime representation. …\nExtract day from underlying NaiveDateTime representation. …\nExtract day from underlying NaiveDateTime representation. …\nExtract day from underlying NaiveDateTime representation. …\nExtract the days from a <code>Duration</code>\nExtract the days from a <code>Duration</code>\nUnpack to <code>ChunkedArray</code> of dtype <code>DataType::Decimal</code>\nUtility for decoding JSON that adds the response value to …\nDecrements the strong reference count on the <code>Arc&lt;T&gt;</code> …\nDecrements the strong reference count on the <code>Arc&lt;T&gt;</code> …\nCreates an empty CStr inside an Arc\nCreates an empty str inside an Arc\nCreates a new <code>Arc&lt;T&gt;</code>, with the <code>Default</code> value for <code>T</code>.\nCreates an empty <code>[T]</code> inside an Arc\nIf true sort in descending order. Default <code>false</code>.\nOrder of the columns. Default all `false``.\nReturn a String describing the optimized logical plan.\nReturn a String describing the optimized logical plan in …\nReturn a String describing the naive (un-optimized) …\nReturn a String describing the naive (un-optimized) …\nDeserializes the statistics in the column chunks from a …\nDiff every sublist.\nCalculate the n-th discrete difference between values.\nThe intersection of a source flags value with the …\nThe intersection of a source flags value with the …\nCompute the dot/inner product between two expressions.\nAttempts to downcast the <code>Arc&lt;dyn Any + Send + Sync&gt;</code> to a …\nDowncasts the <code>Arc&lt;dyn Any + Send + Sync&gt;</code> to a concrete …\nCreates a new <code>Weak</code> pointer to this allocation.\nRemoves columns from the DataFrame. Note that it’s …\nDrops the <code>Arc</code>.\nDrop a column by name. This is a pure method and will …\nRemove a column by name and return the column removed.\nDrop columns that are in <code>names</code>.\nDrop columns that are in <code>names</code> without allocating a <code>HashSet</code>…\nDrop rows containing one or more NaN values.\nDrop NaN values.\nRemoves columns from the DataFrame. Note that it’s …\nDrop rows containing one or more None values.\nDrop null values.\nDrop all null values and return a new Series.\nDrop all null values and return a new Series.\nReturn a new <code>DataFrame</code> where all null values are dropped.\nGet the <code>dt::DateLikeNameSpace</code>\nGet data type of <code>ChunkedArray</code>.\nGet data type of <code>ChunkedArray</code>.\nGet datatype of series.\nGet datatype of series.\nReturns the <code>Field</code>’s <code>ArrowDataType</code>.\nGet the matching <code>DataType</code> for this <code>AnyValue</code>`.\nReturns a reference to the <code>Field</code> datatype.\nIts logical <code>ArrowDataType</code>\nSelect multiple columns by dtype.\nSelect multiple columns by dtype.\nGet the data types of the columns in the <code>DataFrame</code>.\nConstruct a column of <code>Duration</code> from the provided …\nUnpack to <code>ChunkedArray</code> of dtype <code>DataType::Duration</code>\nGet a flags value with all bits unset.\nGet a flags value with all bits unset.\nCreates an empty <code>DataFrame</code> usable in a compile time …\nCreate an empty <code>DataFrame</code> with empty columns as per the …\nCreates an empty <code>DataFrame</code> with a specific <code>height</code>.\nCreate an empty <code>DataFrame</code> with empty columns as per the …\nCheck if strings ends with a substring\nCompare <code>Expr</code> with other <code>Expr</code> on equality.\nEquality for two <code>Arc</code>s.\nCompare <code>Expr</code> with other <code>Expr</code> on equality where <code>None == None</code>…\nEquality where <code>None</code> is treated as UTC.\nCheck for equality.\nCreate a boolean mask by checking for equality.\nCreate a boolean mask by checking for equality.\nCheck for equality where <code>None == None</code>.\nCreate a boolean mask by checking for equality.\nCreate a boolean mask by checking for equality.\nCheck if <code>DataFrame</code>s are equal. Note that <code>None == None</code> …\nCheck if series are equal. Note that <code>None == None</code> …\nCheck if all values in <code>DataFrame</code>s are equal where …\nCheck if all values in series are equal where <code>None == None</code> …\nTotal len divided by max len of first and last non-empty …\nReturns an estimation of the total (heap) allocated size …\nReturns an estimation of the total (heap) allocated size …\nRun any <code>Expr</code> on these lists elements\nStart a window at this interval.\nIf polars may parse matches that not contain the whole …\nExclude a column from a wildcard/regex selection.\nExecutes the given command directly.\nRecursively traverses directories and expands globs if <code>glob</code>…\nRecursively traverses directories and expands globs if <code>glob</code>…\nThis will update <code>scan_args.hive_options.enabled</code> to <code>true</code> if …\nReturns <code>true</code> if <code>expanded_paths</code> were expanded from a single …\nReturn a String describing the logical plan.\nReturns a column with a separate row for every array …\nApply explode operation. See eager explode.\nExplode the String/List column.\nExplode <code>DataFrame</code> to long format by exploding a column …\nExplode a list Series. This expands every item to a new …\nExtend the memory backed by this array with the values …\nThe bitwise or (<code>|</code>) of the bits in each flags value.\nThe bitwise or (<code>|</code>) of the bits in each flags value.\nExtend the memory backed by this <code>DataFrame</code> with the values …\nExtend the memory backed by this array with the values …\nExtend with a constant value.\nExtract the nth capture group from pattern.\nExtract each successive non-overlapping regex match in an …\nExtract each successive non-overlapping regex match in an …\nExtract all capture groups from pattern and return as a …\nExtract a constant usize from an expression.\nUnpack to <code>ChunkedArray</code> of dtype <code>DataType::Float32</code>\nUnpack to <code>ChunkedArray</code> of dtype <code>DataType::Float64</code>\nFetch the result.\nFetch is like a collect operation, but it overwrites the …\nAwait the result synchronously.\nGet field (used in schema)\nGet field (used in schema)\nRetrieve one of the fields of this <code>StructChunked</code> as a new …\nGet access to one of this <code>StructChunked</code>’s fields\nRetrieve one or multiple of the fields of this …\nPer-field overwrites for writing properties.\nReturns the fields of this <code>StructArray</code>.\nGet a reference to the schema fields of the <code>DataFrame</code>.\nFill NaN values in the DataFrame with an expression.\nReplace the floating point <code>NaN</code> values by a value.\nFill None values in the DataFrame with an expression.\nReplace the null values by a value.\nReplace None values with one of the following strategies:\nReplace None values with one of the following strategies:\nReplace None values with a give value <code>T</code>.\nFilter values in the ChunkedArray with a boolean mask.\nFilter by boolean mask. This operation clones data.\nApply a filter predicate, keeping the rows that match it.\nFilter frame rows that match a predicate expression.\nFilter a single column.\nTake the <code>DataFrame</code> rows by a boolean mask.\nReturn the index position of a regular expression …\n<code>str</code> to <code>Categorical</code>\nReturn the index position of a literal substring in the …\nFind the indices of the values where the validity …\nFind the indices of elements where the null masks are …\nTake the SerReader and return a parsed DataFrame.\nFinish builder\nGet the final LazyFrame.\nGet the final LazyFrame.\nRead the file and create the DataFrame.\nTake the SerReader and return a parsed DataFrame.\nWrite the given DataFrame in the writer <code>W</code>. Returns the …\nGet the final LazyFrame. This method assumes, that path is …\nFirst column in a DataFrame.\nGet the first row.\nGet first item of every sublist.\nGet the first value in the group.\nGet the first element of the <code>Series</code> as a <code>Scalar</code>\nGet the first element of the <code>Series</code> as a <code>Scalar</code>\nAggregate grouped <code>Series</code> and find the first value per …\nThe number of chunks for the first column.\nGet the index of the first non null value in this …\nTry get the first path in the scan sources\nAlias for <code>explode</code>.\nAllow Float64 -&gt; Float32\nUsed for <code>DataType::Float64</code> and <code>DataType::Float32</code>.\nAllow Float32 -&gt; Float64\nFloor underlying floating point array to the lowest …\nFloor underlying floating point array to the lowest …\nFloor divide <code>self</code> by <code>rhs</code>.\nOptional parameters for the rolling\nOptional parameters for the rolling\nAccumulate over multiple columns horizontally / row wise.\nForce parallel table evaluation.\nFormatting string\nFormat the results of an array of expressions using a …\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nAllocates a reference-counted slice and fills it by …\nCreates an atomically reference-counted pointer from a …\nAllocates a reference-counted <code>str</code> and copies <code>v</code> into it.\nAllocates a reference-counted <code>str</code> and copies <code>v</code> into it.\nConverts a <code>T</code> into an <code>Arc&lt;T&gt;</code>\nAllocates a reference-counted <code>str</code> and copies <code>v</code> into it.\nConverts a <code>Path</code> into an <code>Arc</code> by copying the <code>Path</code> data into …\nAllocates a reference-counted slice and fills it by …\nConverts a <code>Path</code> into an <code>Arc</code> by copying the <code>Path</code> data into …\nConverts a <code>PathBuf</code> into an Arc&lt;Path&gt; by moving the <code>PathBuf</code> …\nCopies the string into a newly allocated Arc&lt;OsStr&gt;.\nConverts an <code>OsString</code> into an Arc&lt;OsStr&gt; by moving the …\nCopies the string into a newly allocated Arc&lt;OsStr&gt;.\nConverts a <code>CString</code> into an Arc&lt;CStr&gt; by moving the <code>CString</code> …\nMove a boxed object to a new, reference-counted allocation.\nConverts a <code>&amp;CStr</code> into a <code>Arc&lt;CStr&gt;</code>, by copying the contents …\nConverts an atomically reference-counted string slice into …\nConverts a <code>[T; N]</code> into an <code>Arc&lt;[T]&gt;</code>.\nConverts a <code>&amp;mut CStr</code> into a <code>Arc&lt;CStr&gt;</code>, by copying the …\nAllocates a reference-counted slice and moves <code>v</code>’s items …\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nConstruct a new <code>Series</code> from a slice of AnyValues.\nConstruct a new <code>Series</code> with the given <code>dtype</code> from a slice …\nConvert from a bits value.\nConvert from a bits value.\nConvert from a bits value exactly.\nConvert from a bits value exactly.\nConvert from a bits value, unsetting any unknown bits.\nConvert from a bits value, unsetting any unknown bits.\nCreate a <code>CategoricalChunked</code> from a physical array and …\nCreate a <code>CategoricalChunked</code> from an array of <code>idx</code> and an …\nCreate a new <code>ChunkedArray</code> from existing chunks.\nCreate a new <code>ChunkedArray</code> from existing chunks.\nTakes chunks and a polars datatype and constructs the …\nSafety\nSafety\nConstruct a new <code>DurationChunked</code> from an iterator over …\nConstruct a new <code>DurationChunked</code> from an iterator over …\nCreate a <code>CategoricalChunked</code> from a categorical indices. …\nTakes each element in the <code>Iterator</code> and collects it into an …\nThe bitwise or (<code>|</code>) of the bits in each flags value.\nThe bitwise or (<code>|</code>) of the bits in each flags value.\nPanics\nPanics\nCreate a list-array from an iterator. Used in group_by …\nSafety\nCreate a list-array from an iterator. Used in group_by …\nCreate a new ChunkedArray from an iterator.\nCreate a list-array from an iterator. Used in group_by …\nCreate a list-array from an iterator. Used in group_by …\nCreate a new ChunkedArray from an iterator.\nCreate a new ChunkedArray from an iterator.\nCreate a new ChunkedArray from an iterator.\nCreate a new ChunkedArray from an iterator.\nCreate a new ChunkedArray from an iterator.\nCreate a new ChunkedArray from an iterator.\nConstruct a new <code>DateChunked</code> from an iterator over <code>NaiveDate</code>…\nConstruct a new <code>DateChunked</code> from an iterator over optional …\nConstruct a new <code>DatetimeChunked</code> from an iterator over …\nConstruct a new <code>TimeChunked</code> from an iterator over <code>NaiveTime</code>…\nConstruct a new <code>TimeChunked</code> from an iterator over optional …\nGet a flags value with the bits of a flag with the given …\nGet a flags value with the bits of a flag with the given …\nThis is the recommended way to create a json reader as …\nConvert a non-logical <code>ArrayChunked</code> back into a logical …\nConvert a non-logical <code>StructChunked</code> back into a logical …\nConvert a non-logical <code>ListChunked</code> back into a logical …\nSafety\nConvert a non-logical series back into a logical series …\nConstructs an <code>Arc&lt;T&gt;</code> from a raw pointer.\nConstructs an <code>Arc&lt;T, A&gt;</code> from a raw pointer.\nCreate a new <code>DataFrame</code> from rows. This should only be used …\nCreate a new <code>DataFrame</code> from rows.\nCreate a new <code>DataFrame</code> from an iterator over rows.\nCreate a key value metadata object from a static key value …\nConstruct from a static string.\nCreate a <code>CategoricalChunked</code> from a fixed list of …\nInitialize by name and values.\nCreate a new ChunkedArray by taking ownership of the Vec. …\nCreate a new ChunkedArray from a Vec and a validity mask.\nCreate a ChunkedArray with a single value.\nFull outer join this query with another lazy query.\nPerform a full outer join on two DataFrames\nCreate a new <code>DataFrame</code> with the given schema, only …\nThe function implementation.\nA function that cannot be expressed with <code>map</code> or <code>apply</code> and …\nTake the values by idx.\nTraverse and collect every nth element in a new array.\n‘Greater than or equal to’ comparison for two <code>Arc</code>s.\nGet items in every sub-array by index.\nGet items in every sublist by index.\nTake the values by a single index.\nGet a single value from this <code>ChunkedArray</code>. If the return …\nGet a single value by index. Don’t use this operation …\nGet a single value by index. Don’t use this operation …\nGet the scan source at specific address\n<code>Categorical</code> to <code>str</code>\nGet a row in the <code>DataFrame</code>. Beware this is slow.\nGet a single value. Beware this is slow.\nGets <code>AnyValue</code> from <code>LogicalType</code>\nGets <code>AnyValue</code> from <code>LogicalType</code>\nGets AnyValue from LogicalType\nGet a single value. Beware this is slow. If you need to …\nSafety\nSafety\nGet the categories in this <code>RevMapping</code>\nGet column index of a <code>Series</code> by name.\nExample\nGet the <code>Vec&lt;PlSmallStr&gt;</code> representing the column names.\nGet a reference to the <code>DataFrame</code> columns.\nGet mutable access to the underlying columns.\nGet current optimizations.\nGetter for the <code>DataType</code> of the value\nreturns the bounds for the earliest window bounds that …\nReturns the fields the <code>DataType::Struct</code>.\nGet the index of the first occurrence of a glob symbol.\nGet the internal representation of the GroupBy operation. …\nGet the internal representation of the GroupBy operation. …\nGet the inner values as <code>Series</code>, ignoring the list offsets.\nGet the inner values as <code>Series</code>\nRecurse nested types until we are at the leaf array.\nRecurse nested types until we are at the leaf array.\nFetch and memoize the metadata of the parquet file.\nReturns a mutable reference into the given <code>Arc</code>, if there …\nReturns a mutable reference into the given <code>Arc</code>, without …\nGet a hold to an object that can be formatted or …\nGet the value at this index as a downcastable Any trait …\nGet the value at this index as a downcastable Any trait …\nGet the value at this index as a downcastable Any trait …\nGet the value at this index as a downcastable Any trait …\nGet a hold to an object that can be formatted or …\nGet a reference to the mapping of categorical types to the …\nGet a row from a <code>DataFrame</code>. Use of this is discouraged as …\nAmortize allocations by reusing a row. The caller is …\nAmortize allocations by reusing a row. The caller is …\nGet the full shape of a multidimensional array.\nReturns the DataType variant associated with this …\nGet a reference to the <code>&amp;str</code> contained within <code>AnyValue</code>.\nGet the supertype of the columns in this DataFrame\nSafety\nGet a single value by index. Don’t use this operation …\nSafety\nSafety\nGet a single value from this <code>ChunkedArray</code>. If the return …\nSafety\nExpand path given via globbing rules.\nPerforms a “group-by” on a <code>LazyFrame</code>, producing a …\nGroup DataFrame using a Series column.\nGroup based on a time value (or index value of type Int32, …\nSimilar to <code>group_by</code>, but order of the DataFrame is …\nGroup DataFrame using a Series column. The groups are …\nDifferent from <code>group_by_windows</code>, where define window …\nWindow boundaries are created based on the given <code>Window</code>, …\nCreate the tuples need for a group_by operation. * The …\nCreate the tuples need for a group_by operation. * The …\nUse the indexes as perfect groups.\nGet the group_by group indexes.\nGreater than comparison.\nCheck if <code>Expr</code> &gt; <code>Expr</code>.\nGreater-than comparison for two <code>Arc</code>s.\nCreate a boolean mask by checking if self &gt; rhs.\nCreate a boolean mask by checking if self &gt; rhs.\nGreater than or equal comparison.\nCheck if <code>Expr</code> &gt;= <code>Expr</code>.\nCreate a boolean mask by checking if self &gt;= rhs.\nCreate a boolean mask by checking if self &gt;= rhs.\nReturn if any the chunks in this <code>ChunkedArray</code> have nulls.\nReturn if any the chunks in this <code>ChunkedArray</code> have nulls.\nReturn first n rows of each group\nGet the head of every sublist\nGet the first <code>n</code> elements of the Expr result.\nGet the head of the <code>ChunkedArray</code>\nGet the head of the <code>DataFrame</code>.\nGet the head of the Series.\nGet the height of the <code>DataFrame</code> which is the number of …\nHold the StringCache\nHorizontally concatenate all strings.\nExtract hour from underlying NaiveDateTime representation. …\nExtract hour from underlying NaiveDateTime representation. …\nExtract hour from underlying NaiveDateTime representation. …\nExtract hour from underlying NaiveDateTime representation. …\nExtract hour from underlying NaiveDateTime representation. …\nExtract hour from underlying NaiveDateTime representation. …\nExtract the hours from a <code>Duration</code>\nExtract the hours from a <code>Duration</code>\nSelect the join type.\nAdd multiple <code>Series</code> to a <code>DataFrame</code>. The added <code>Series</code> are …\nAdd multiple <code>Column</code> to a <code>DataFrame</code>. Errors if the …\nAdd columns horizontally.\nUnpack to <code>ChunkedArray</code> of dtype <code>DataType::Int128</code>\nUnpack to <code>ChunkedArray</code> of dtype <code>DataType::Int16</code>\nUnpack to <code>ChunkedArray</code>\nUnpack to <code>ChunkedArray</code> of dtype <code>DataType::Int64</code>\nUnpack to <code>ChunkedArray</code> of dtype <code>DataType::Int8</code>\nTurn the <code>ScanSources</code> into some kind of identifier\nGet a mutable reference to the <code>GroupsIdx</code>.\nIf <code>ambiguous</code> is length-1 and not equal to “null”, we …\nGroupBy the group to a Series.\nConvert the values of this Series to a ListChunked with a …\nSet whether to write UTF-8 BOM.\nAdd the boundaries to the DataFrame.\nSet whether to write headers.\nIncrements the strong reference count on the <code>Arc&lt;T&gt;</code> …\nIncrements the strong reference count on the <code>Arc&lt;T&gt;</code> …\nSelect multiple columns by index.\nTime or index column.\nTime or index column.\nInfer the schema of a CSV file by reading through the …\nInfers a <code>ArrowSchema</code> from parquet’s <code>FileMetadata</code>.\nSet the JSON reader to infer the schema of the file. …\nGet the inner data type of the fixed size list.\nGet the inner data type of the list.\nGet the inner data type of a nested type.\nInner join this query with another lazy query.\nPerform an inner join on two DataFrames.\nThe bitwise or (<code>|</code>) of the bits in two flags values.\nThe bitwise or (<code>|</code>) of the bits in two flags values.\nInsert a new column at a given index.\nGenerate a range of integers.\nGenerate a range of integers for each row of the input …\nAllow casting when target dtype is lossless supertype\nInterpolate intermediate values. Nulls at the beginning …\nInterpolate intermediate values. Nulls at the beginning …\nThe bitwise and (<code>&amp;</code>) of the bits in two flags values.\nThe bitwise and (<code>&amp;</code>) of the bits in two flags values.\nWhether any set bits in a source flags value are also set …\nWhether any set bits in a source flags value are also set …\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nConverts the reference-counted slice into a …\nTurn a scalar into a column with <code>length=1</code>.\nDeconstructs the <code>StructArray</code> into its individual …\nReturns the inner value, if the <code>Arc</code> has exactly one strong …\nTurn <code>Column</code> into a <code>Column::Series</code>.\nTry cast the scan sources to <code>ScanSources::Paths</code> with a …\nGet the physical array (the category indexes).\nConsumes the <code>Arc</code>, returning the wrapped pointer.\nConsumes the <code>Arc</code>, returning the wrapped pointer and …\nCreates a CSV reader using a file handle.\nTry to coerce to an AnyValue with static lifetime. This …\nWhether all known bits in this flags value are set.\nWhether all known bits in this flags value are set.\nCheck if this <code>DataType</code> is an array.\nCheck if this <code>DataType</code> is a boolean.\nCheck if the path is a cloud url.\nIs the first path a cloud URL?\nCheck if this <code>DataType</code> is a Decimal type (of any …\nGet a mask of all the duplicated rows in the <code>DataFrame</code>.\nCheck if ChunkedArray is empty.\nCheck if Series is empty.\nCheck if Series is empty.\nWhether all bits in this flags value are unset.\nWhether all bits in this flags value are unset.\nReturns <code>true</code> if the <code>DataFrame</code> contains no rows.\nGet mask of finite values if dtype is Float.\nCheck if numeric value is finite\nGet a mask of the first unique value.\nCheck if this <code>DataType</code> is a basic floating point type …\nReturns whether the duration consists of full days.\nCheck if the values of the left expression are in the …\nGet mask of infinite values if dtype is Float.\nCheck if numeric value is infinite\nCheck if this <code>DataType</code> is an integer. Note, this also …\nCheck if the whole dtype is known.\nGet a mask of the last unique value.\nExtract year from underlying NaiveDate representation. …\nExtract year from underlying NaiveDate representation. …\nExtract year from underlying NaiveDate representation. …\nExtract year from underlying NaiveDate representation. …\nExtract year from underlying NaiveDateTime representation. …\nExtract year from underlying NaiveDateTime representation. …\nCheck if this <code>DataType</code> is a list.\nCheck if this <code>DataType</code> is a logical type\nGet mask of NaN values if dtype is Float.\nCheck if numeric value is NaN (note this is different than …\nGet inverse mask of NaN values if dtype is Float.\nCheck if numeric value is NaN (note this is different than …\nA column which is <code>false</code> wherever <code>expr</code> is null, <code>true</code> …\nGet a mask of the non-null values.\nRun is_not_null operation on <code>Expr</code>.\nGet a mask of the valid values.\nA column which is <code>true</code> wherever <code>expr</code> is null, <code>false</code> …\nGet a mask of the null values.\nRun is_null operation on <code>Expr</code>.\nGet a mask of the null values.\nIts nullability\nReturns true if contains a single chunk and has no null …\nCheck if type is sortable\nAre the sources all paths?\nMay give false negatives because it ignores the null …\nMay give false negatives because it ignores the null …\nCheck if datatype is a primitive type. By that we mean that\nCheck if this <code>DataType</code> is a primitive numeric type …\nChecks if a <code>Series</code> is sorted. Tries to fail fast.\nCheck if this <code>DataType</code> is a struct\nCheck if this <code>DataType</code> is a temporal type\nGet a mask of all the unique rows in the <code>DataFrame</code>.\n<code>true</code> if zero duration.\nThis year number might not match the calendar year number.\nThis year number might not match the calendar year number.\nReturns an iterator of <code>Option&lt;Box&lt;dyn Array&gt;&gt;</code>\nYield a set of contained flags values.\nYield a set of contained flags values.\nIterator over the columns as <code>Series</code>.\niterate over <code>Series</code> as <code>AnyValue</code>.\nIterator over the rows in this <code>DataFrame</code> as Arrow …\nIterator over the rows in this <code>DataFrame</code> as Arrow …\nYield a set of contained named flags values.\nYield a set of contained named flags values.\nCreate an <code>Iterator</code> that iterates over the <code>&amp;str</code> values of …\nGet the buffer of bits representing null values\nJoin all string items in a sub-array and place a separator …\nGeneric function to join two LazyFrames.\nJoin all string items in a sublist and place a separator …\nGeneric join method. Can be used to join on multiple …\nThis is similar to a left-join except that we match on …\nConsume <code>self</code> and return a <code>JoinBuilder</code> to customize a join …\nJoin on null values. By default null values will never …\nKeep the original root name\nWhich rows to keep.\nreturns the metadata\nCustom file-level key value metadata\nkey_value_metadata of this file.\nTruncate the time column values to the window.\nLast column in a DataFrame.\nGet the last row.\nGet last item of every sublist.\nGet the last value in the group.\nGet the last element of the <code>Series</code> as a <code>Scalar</code>\nGet the last element of the <code>Series</code> as a <code>Scalar</code>\nAggregate grouped <code>Series</code> and return the last value per …\nGet the index of the last non null value in this …\nConvert the <code>DataFrame</code> into a <code>LazyFrame</code>\n‘Less than or equal to’ comparison for two <code>Arc</code>s.\nGet the absolute inner data type of a nested type.\nLeft outer join this query with another lazy query.\nPerform a left outer join on two DataFrames\nThe expressions you want to join the left table on.\nReturn the number of rows in the context.\nGet length of series.\nReturn the number of elements in each list.\nGet the length of the ChunkedArray\nGet the length of the <code>RevMapping</code>\nApply lhs / self\nApply lhs % self\nApply lhs - self\nLimit the DataFrame to the first <code>n</code> rows.\nTake a view of top n elements\nTake <code>num_elements</code> from the top as a zero copy view.\nTake <code>num_elements</code> from the top as a zero copy view.\nLimit a sort output, this is for optimization purposes and …\nLimit a sort output, this is for optimization purposes and …\nString appended after every row.\nGenerate a series of equally-spaced points.\nCreate a column of linearly-spaced sequences from ‘start…\nGet the <code>list::ListNameSpace</code>\nUnpack to <code>ChunkedArray</code> of dtype list\nTODO: Move this somewhere else?\nCreate a Literal Expression from <code>L</code>. A literal expression …\nLiteral expression.\nLogical “and” operation.\nLogical “or” operation.\nReduce memory usage at the expense of performance\nReduce memory consumption at the expense of performance\nGet minimal value that could be hold by this dtype.\nGet the value by index in the sublists. So index <code>0</code> would …\nIn case the inner dtype <code>DataType::String</code>, the individual …\nLess than comparison.\nCheck if <code>Expr</code> &lt; <code>Expr</code>.\nLess-than comparison for two <code>Arc</code>s.\nCreate a boolean mask by checking if self &lt; rhs.\nCreate a boolean mask by checking if self &lt; rhs.\nLess than or equal comparison\nCheck if <code>Expr</code> &lt;= <code>Expr</code>.\nCreate a boolean mask by checking if self &lt;= rhs.\nCreate a boolean mask by checking if self &lt;= rhs.\nWhether to preserve the row order.\nThe output file needs to maintain order of the data that …\nIf true maintain the order of equal elements. Default <code>false</code>…\nWhether maintain the order of equal elements. Default <code>false</code>…\nThis will maintain the order of the input. Note that this …\nMakes a mutable reference into the given <code>Arc</code>.\nApply a function/closure once the logical plan get …\nDefine an alias by mapping a function over the original …\nApply a function/closure once the logical plan get …\nApply a closure on the two columns that are evaluated from …\nSet the timezone of a datetime dtype.\nMap a single dtype.\nApply a function/closure once the logical plan get …\nApply a function/closure over multiple columns once the …\nMap to a float supertype if numeric, else preserve\nRe-export to shorten code.\nApply a function to the parse options.\nMap to a float supertype.\nMap the dtype to the dtype of the list/array elements.\nMap the dtypes to the “supertype” of a list of lists.\nMap the dtype to the “supertype” of all fields.\nMatch or evolve to a certain schema.\nAnswers if this type matches the given type of a schema.\nMaterialize this datatype if it is unknown. All other …\nFind the maximum of all the values in the column named <code>name</code>…\nCompute the maximum of the items in every subarray.\nAggregate all the columns as their maximum values.\nCompute the maximum of the items in every sublist.\nReturns the maximum value in the array, according to the …\nReturns the maximum value in the array, according to the …\nReduce groups to maximum value.\nTry to get the maximum value for this datatype.\nAggregate grouped series and compute the maximum value per …\nReturns the maximum value in the array, according to the …\nAggregate the column horizontally to their max values.\nThe highest number of chunks for any column.\nGet the max of the <code>ChunkedArray</code> as a new <code>Series</code> of length …\nGet the max of the Series as a new Series of length 1.\nGet the max of the Series as a new Series of length 1.\nMax row group height, useful for sharing column …\nFind the mean of all the values in the column named <code>name</code>. …\nAggregate all the columns as their mean values.\nCompute the mean of every sublist and return a <code>Series</code> of …\nReturns the mean value in the array. Returns <code>None</code> if the …\nReturns the mean value in the array. Returns <code>None</code> if the …\nReduce groups to the mean value.\nReturns the mean value in the array Returns an option …\nReturns the mean value in the array Returns an option …\nAggregate grouped series and compute the mean per group.\nCompute the mean of all numeric values horizontally across …\nFind the median of all the values in the column named <code>name</code>…\nCompute the median of the items in every subarray.\nAggregate all the columns as their median values.\nReturns the mean value in the array. Returns <code>None</code> if the …\nReturns the mean value in the array. Returns <code>None</code> if the …\nReduce groups to the median value.\nReturns the median value in the array Returns an option …\nReturns the median value in the array Returns an option …\nAggregate grouped <code>Series</code> and determine the median per …\nGet the median of the <code>ChunkedArray</code> as a new <code>Series</code> of …\nGet the median of the Series as a new Series of length 1.\nGet the median of the Series as a new Series of length 1.\nSet if the file is to be memory_mapped. Only works with …\nAdditional custom (opaque) metadata.\nExtract the microseconds from a <code>Duration</code>\nExtract the microseconds from a <code>Duration</code>\nCalculate the millennium from the underlying NaiveDateTime …\nCalculate the millennium from the underlying NaiveDateTime …\nExtract the milliseconds from a <code>Duration</code>\nExtract the milliseconds from a <code>Duration</code>\nFind the minimum of all the values in the column named <code>name</code>…\nCompute the minimum of the items in every subarray.\nAggregate all the columns as their minimum values.\nCompute the minimum of the items in every sublist.\nReduce groups to minimal value.\nTry to get the minimum value for this datatype.\nAggregate grouped series and compute the minimal value per …\nReturns the minimum value in the array, according to the …\nAggregate the column horizontally to their min values.\nAmount of elements in the window that should be filled …\nAmount of elements in the window that should be filled …\nGet the min of the <code>ChunkedArray</code> as a new <code>Series</code> of length …\nGet the min of the Series as a new Series of length 1.\nGet the min of the Series as a new Series of length 1.\nExtract minute from underlying NaiveDateTime …\nExtract minute from underlying NaiveDateTime …\nExtract minute from underlying NaiveDateTime …\nExtract minute from underlying NaiveDateTime …\nExtract minute from underlying NaiveDateTime …\nExtract minute from underlying NaiveDateTime …\nExtract the minutes from a <code>Duration</code>\nExtract the seconds from a <code>Duration</code>\nRecursively create all the directories in the path.\nCreate a temporary <code>ChunkedArray</code> from a slice.\nCreate a temporary <code>ChunkedArray</code> from a slice.\nCompute the mode(s) of this column. This is the most …\nExtract month from underlying NaiveDateTime representation.\nExtract month from underlying NaiveDateTime representation.\nExtract month from underlying NaiveDateTime representation.\nExtract month from underlying NaiveDateTime representation.\nExtract month from underlying NaiveDateTime representation.\nExtract month from underlying NaiveDateTime representation.\nIf true sort in multiple threads. Default <code>true</code>.\nWhether sort in multiple threads. Default <code>true</code>.\nNumber of chunks in this Series\nNumber of chunks in this Series\nTry to stop parsing when <code>n</code> rows are parsed. During …\nTry to stop parsing when <code>n</code> rows are parsed. During …\nTry to stop parsing when <code>n</code> rows are parsed. During …\nNumber of unique values in the <code>ChunkedArray</code>\nNumber of unique values in the <code>ChunkedArray</code>\nGet the number of unique values in the groups.\nGet unique values in the Series.\nGet unique values in the Series.\nAggregate grouped <code>Series</code> by counting the number of unique …\nName of series.\nGet the <code>name::ExprNameNameSpace</code>\nName of the <code>ChunkedArray</code>.\nReturns a reference to the <code>Field</code> name.\nname\nIts name\nReduce groups to maximum value.\nReduce groups to minimal value.\nExtract second from underlying NaiveDateTime …\nExtract second from underlying NaiveDateTime …\nExtract second from underlying NaiveDateTime …\nReturns the number of nanoseconds since the whole non-leap …\nReturns the number of nanoseconds since the whole non-leap …\nExtract second from underlying NaiveDateTime …\nExtract the nanoseconds from a <code>Duration</code>\nReturns the nanoseconds from the <code>Duration</code> without the …\nExtract the nanoseconds from a <code>Duration</code>\nInequality for two <code>Arc</code>s.\nReturns whether duration is negative.\nTranslate the negative index to an offset.\nCompare <code>Expr</code> with other <code>Expr</code> on non-equality.\nCompare <code>Expr</code> with other <code>Expr</code> on non-equality where …\nCreate a new instance of the <code>SerReader</code>\nInitialize by name and values.\nCreate the <code>JoinBuilder</code> with the provided <code>LazyFrame</code> as the …\nCreate a new CsvReader from a file/stream using default …\nCreate a new integer size <code>Duration</code>\nCreate a new <code>JsonWriter</code> writing to <code>buffer</code> with format …\nCreate a new JsonLineReader from a file/ stream\nCreate a new <code>ParquetReader</code> from an existing <code>Reader</code>.\nCreate a new writer\nConstructs a new <code>Arc&lt;T&gt;</code>.\nConstruct a new <code>DatetimeArgs</code> set to <code>year</code>, <code>month</code>, <code>day</code>\nCreate a new <code>DurationArgs</code> with all fields set to <code>lit(0)</code>. …\nReturns a new <code>StructArray</code>\nCreate <code>SortOptions</code> with default values.\nCreate <code>SortMultipleOptions</code> with default values.\nCreate a new UserDefinedFunction\nCreates a new <code>Field</code>.\nCreates a new <code>Field</code>.\nCreate a DataFrame from a Vector of Series.\nConstruct a new <code>Series</code> from a collection of <code>AnyValue</code>.\nConstructs a new <code>Arc&lt;T&gt;</code> while giving you a <code>Weak&lt;T&gt;</code> to the …\nConstructs a new <code>Arc&lt;T, A&gt;</code> in the given allocator while …\nCreates an empty <code>StructArray</code>.\nCreate a new empty Series.\nCreate a new ChunkedArray filled with values at that index.\nCreate a new Series filled with values from the given …\nConstruct a date ChunkedArray from individual time …\nConstruct a date ChunkedArray from individual time …\nConstruct a datetime ChunkedArray from individual time …\nConstruct a datetime ChunkedArray from individual time …\nCreates a new <code>CommentPrefix</code> from a <code>&amp;str</code>.\nConstructs a new <code>Arc&lt;T&gt;</code> in the provided allocator.\nCreates a new <code>CommentPrefix</code> for the <code>Multi</code> variant.\nCreate a new <code>DataFrame</code> but does not check the length or …\nCreate a new <code>DataFrame</code> but does not check the length or …\nCreates a null <code>StructArray</code> of length <code>length</code>.\nCreates a new <code>CommentPrefix</code> for the <code>Single</code> variant.\nSafety\nConstructs a new <code>Arc</code> with uninitialized contents.\nConstructs a new <code>Arc</code> with uninitialized contents in the …\nConstructs a new atomically reference-counted slice with …\nConstructs a new atomically reference-counted slice with …\nSpecialization that prevents an allocation prefer this …\nConverts a sequence of columns into a DataFrame, …\nConverts a sequence of columns into a DataFrame, …\nConverts a sequence of columns into a DataFrame, …\nCreate a new <code>ChunkedArray</code> and explicitly set its <code>length</code> …\nConstructs a new <code>Arc</code> with uninitialized contents, with the …\nConstructs a new <code>Arc</code> with uninitialized contents, with the …\nConstructs a new atomically reference-counted slice with …\nConstructs a new atomically reference-counted slice with …\nProduce the next batch Polars can consume. Implement this …\nConvert missing values to <code>NaN</code> values.\nNegates a boolean column.\nNegate <code>Expr</code>.\nThe bitwise negation (<code>!</code>) of the bits in a flags value, …\nThe bitwise negation (<code>!</code>) of the bits in a flags value, …\nCheck for inequality.\nCreate a boolean mask by checking for inequality.\nCreate a boolean mask by checking for inequality.\nCheck for inequality where <code>None == None</code>.\nCreate a boolean mask by checking for inequality.\nCreate a boolean mask by checking for inequality.\nNth column in a DataFrame.\nUnpack to <code>ChunkedArray</code> of dtype <code>DataType::Null</code>\nNull value representation.\nCount the null values.\nAggregate all the columns as the sum of their null value …\nGet the null count of the column/group.\nReturn the number of null values in the ChunkedArray.\nCreate a new <code>DataFrame</code> that shows the null counts per …\nWhether place null values last. Default <code>false</code>.\nWhether place null values last. Default <code>false</code>.\nNumber of rows in the parquet file.\nNumber of rows in the parquet file.\nnumber of rows in the file.\nOffset window boundaries.\nThe expressions you want to join both tables on.\nConverts timezones to canonical form.\nOptions for the function.\nBitwise “or” operation.\nGet the bitwise OR of the Series as a new Series of length …\nGet the bitwise OR of the Series as a new Series of length …\nReturns the day of year starting from 1.\nReturns the day of year starting from 1.\nReturns the day of year starting from 1.\nReturns the day of year starting from 1.\nReturns the day of year starting from 1.\nReturns the day of year starting from 1.\nExtract ordinal year from underlying NaiveDateTime …\nExtract ordinal year from underlying NaiveDateTime …\nDefine a default for the <code>when-then-otherwise</code> expression.\nDefine a default for the <code>when-then-otherwise</code> expression.\nApply window function over a subgroup. This is similar to …\nApply a closure over the groups as a new <code>DataFrame</code> in …\nParse a string into a <code>Duration</code>\nPartial comparison for two <code>Arc</code>s.\nOnly implemented for the same types and physical types!\nSplit into multiple DataFrames partitioned by groups\nSplit into multiple DataFrames partitioned by groups Order …\nWindow duration.\nWindow duration.\nGet a reference to the physical array (the categories).\nConstant Pi\nConstructs a new <code>Pin&lt;Arc&lt;T&gt;&gt;</code>. If <code>T</code> does not implement <code>Unpin</code>…\nConstructs a new <code>Pin&lt;Arc&lt;T, A&gt;&gt;</code> in the provided allocator. …\nPipe different functions/ closure operations that work on …\nPipe different functions/ closure operations that work on …\nPipe different functions/ closure operations that work on …\nRemoves the last <code>Series</code> from the <code>DataFrame</code> and returns it, …\nRaise expression to the power <code>exponent</code>\nSlice applied before predicates\nallow predicate pushdown optimizations\nAdd a prefix to the root column name.\nPrepare the given <code>DslPlan</code> for execution on Polars Cloud.\nGet the product of the <code>ChunkedArray</code> as a new <code>Series</code> of …\nGet the product aggregation of an expression.\nGet the product of an array.\nProfile a LazyFrame.\nSelect fields using a bitmap.\nallow projection pushdown optimizations\nPropagate nulls of nested datatype to all levels of …\nPropagate down nulls in nested types.\nPropagate down nulls in nested types.\nRemove empty chunks.\nReturns <code>true</code> if the two <code>Arc</code>s point to the same allocation …\nFind a specific quantile of all the values in the column …\nAggregate all the columns as their quantile values.\nAggregate a given quantile of the ChunkedArray. Returns …\nAggregate a given quantile of the ChunkedArray. Returns …\nCompute the quantile per group.\nAggregate grouped <code>Series</code> and determine the quantile per …\nGet the quantile of the <code>ChunkedArray</code> as a new <code>Series</code> of …\nGet the quantile of the ChunkedArray as a new Series of …\nGet the quantile of the ChunkedArray as a new Series of …\nExtract month from underlying NaiveDateTime representation.\nExtract month from underlying NaiveDateTime representation.\nExtract quarter from underlying NaiveDateTime …\nExtract quarter from underlying NaiveDateTime …\nExtract quarter from underlying NaiveDateTime …\nExtract quarter from underlying NaiveDateTime …\nQueues the given command for further execution.\nQuoting character.\nWhen to insert quotes.\nCreate <code>ChunkedArray</code> with samples from a Bernoulli …\nCreate <code>ChunkedArray</code> with samples from a Normal …\nCreate <code>ChunkedArray</code> with samples from a Standard Normal …\nCreate <code>ChunkedArray</code> with samples from a Uniform …\nAssign ranks to data, dealing with ties appropriately.\nRead the parquet file in parallel (default). The single …\nRechunk the memory to contiguous chunks when parsing is …\nAggregate all chunks to a contiguous array of memory.\nRechunks this ChunkedArray, returning a new Cow::Owned …\nRechunks this ChunkedArray in-place.\nRechunks all columns to only have a single chunk.\nRechunks all columns to only have a single chunk and turns …\nAnalogous to <code>Iterator::reduce</code>.\nGet a reference to the field.\nRegisters a value to a categorical index without pushing …\nRemove rows matching a filter predicate (note that rows …\nRemove frame rows that match a predicate expression.\nThe intersection of a source flags value with the …\nThe intersection of a source flags value with the …\nRename the Series.\nRename columns in the DataFrame.\nRename this <code>ChunkedArray</code>.\nRename a column in the <code>DataFrame</code>.\nRename series.\nRename the fields of the <code>StructChunked</code>.\nCreate a column of length <code>n</code> containing <code>n</code> copies of the …\nRepeat the column <code>n</code> times, where <code>n</code> is determined by the …\nReplace values by different values of the same data type.\nReplace the leftmost regex-matched (sub)string with …\nReplace the given values with other values.\nReplace a column with a <code>Series</code>.\nReplace all regex-matched (sub)strings with another string\nReplace column at index <code>idx</code> with a <code>Series</code>.\nReplace specific time component of a <code>DateChunked</code> with a …\nReplace specific time component of a <code>DatetimeChunked</code> with …\nReplace the leftmost literal (sub)string with another …\nReplace all matching literal (sub)strings with another …\nReplace or update a column. The difference between this …\nReplace all values by different values.\nReplace all values by different values.\nReplace the given values with other values.\nReplaces a “~” in the Path with the home directory.\nThe function output type.\nReturn a reversed version of this array.\nreturn a Series in reversed order\nReverse the <code>DataFrame</code> from top to bottom.\nReverse every sublist\nReverse column\nGet a <code>DataFrame</code> with all the columns in reversed order.\nThe expressions you want to join the right table on.\nGet the lengths of runs of identical values.\nGet the lengths of runs of identical values.\nSimilar to <code>rle</code>, but maps values to run IDs.\nSimilar to <code>rle</code>, but maps values to run IDs.\nGet the run-Lengths of values.\nCreate rolling groups based on a time column.\nApply a custom function over a rolling/ moving window of …\nApply a rolling custom function. This is pretty slow …\nApply a custom function over a rolling/ moving window of …\nApply a custom function over a rolling/ moving window of …\nApply a custom function over a rolling/ moving window of …\nApply a rolling custom function. This is pretty slow …\nApply a rolling max to a Series.\nApply a rolling max to a Series.\nApply a rolling maximum.\nApply a rolling max to a Series based on another Series.\nApply a rolling max to a Series based on another Series.\nApply a rolling maximum based on another column.\nApply a rolling mean to a Series.\nApply a rolling mean to a Series.\nApply a rolling mean.\nApply a rolling mean to a Series based on another Series.\nApply a rolling mean to a Series based on another Series.\nApply a rolling mean based on another column.\nApply a rolling median.\nApply a rolling median based on another column.\nApply a rolling min to a Series.\nApply a rolling min to a Series.\nApply a rolling minimum.\nApply a rolling min to a Series based on another Series.\nApply a rolling min to a Series based on another Series.\nApply a rolling minimum based on another column.\nApply a rolling quantile to a Series.\nApply a rolling quantile to a Series.\nApply a rolling quantile.\nApply a rolling quantile to a Series based on another …\nApply a rolling quantile to a Series based on another …\nApply a rolling quantile based on another column.\nApply a rolling std_dev to a Series.\nApply a rolling std_dev to a Series.\nApply a rolling std-dev.\nApply a rolling std_dev to a Series based on another …\nApply a rolling std_dev to a Series based on another …\nApply a rolling std-dev based on another column.\nApply a rolling sum to a Series.\nApply a rolling sum to a Series.\nApply a rolling sum.\nApply a rolling sum to a Series based on another Series.\nApply a rolling sum to a Series based on another Series.\nApply a rolling sum based on another column.\nApply a rolling variance to a Series.\nApply a rolling variance to a Series.\nApply a rolling variance.\nApply a rolling variance to a Series based on another …\nApply a rolling variance to a Series based on another …\nApply a rolling variance based on another column.\nRound underlying floating point array to given decimal.\nRound underlying floating point array to given decimal …\nRound the given ms timestamp by the window boundary.\nRound the given ns timestamp by the window boundary.\nRound to a number of significant figures.\nRound the given us timestamp by the window boundary.\nIf <code>None</code> will be all written to a single row group.\nThe row groups of this file\nAdd a row index column.\nReturn the row index settings.\nAdd a row index column.\nProxy of the number of rows in both sides of the joins …\nCheck if the categoricals have a compatible mapping\nSample a fraction between 0.0-1.0 of this <code>ChunkedArray</code>.\nSample a fraction between 0.0-1.0 of this <code>DataFrame</code>.\nSample a fraction between 0.0-1.0 of this <code>ChunkedArray</code>.\nSample n datapoints from this <code>ChunkedArray</code>.\nSample n datapoints from this <code>DataFrame</code>.\nCreates a DataFrame from the supplied function &amp; scan …\nCreate a LazyFrame directly from a ipc scan.\nCreate a LazyFrame directly from a parquet scan.\nCreate a LazyFrame directly from a parquet scan.\nCreate a LazyFrame directly from a parquet scan.\nInvariant for implementations: if the scatter() fails, …\nSet the values at indexes <code>idx</code> to some optional value …\nSet the values at indexes <code>idx</code> by applying a closure to …\nGet arrow schema of the Ipc File.\nGet schema of the Ipc Stream File\n<code>Schema</code> of the file.\nfunction to supply the schema. Allows for an optional …\nReturns the <code>SchemaDescriptor</code> that describes schema of this …\nGet the <code>DataFrame</code> schema.\nUser-provided schema of the file. Will be inferred during …\nschema descriptor.\nCheck if <code>DataFrame</code>’ schemas are equal.\nExtract second from underlying NaiveDateTime …\nExtract second from underlying NaiveDateTime …\nExtract second from underlying NaiveDateTime …\nExtract second from underlying NaiveDateTime …\nExtract second from underlying NaiveDateTime …\nExtract second from underlying NaiveDateTime …\nExtract the seconds from a <code>Duration</code>\nExtract the seconds from a <code>Duration</code>\nSelect (and optionally rename, with <code>alias</code>) columns from …\nSelect the column(s) that should be aggregated. You can …\nSelect column(s) from this <code>DataFrame</code> and return a new …\nSelect a <code>Series</code> by index.\nSelect column(s) from this <code>DataFrame</code> by range and return a …\nSelect column(s) from this <code>DataFrame</code> and return them into …\nSelect with a known schema. The schema names must match …\nSelect with a known schema without checking for duplicates …\nLeft semi join this query with another lazy query.\nUsed as separator.\nSet the values where the mask evaluates to <code>true</code> to some …\nCall <code>insert</code> when <code>value</code> is <code>true</code> or <code>remove</code> when <code>value</code> is …\nCall <code>insert</code> when <code>value</code> is <code>true</code> or <code>remove</code> when <code>value</code> is …\nSet the column names.\nSets custom schema metadata. Must be called before <code>start</code> …\nSets custom schema metadata. Must be called before <code>start</code> …\nReturns whether the flags were set\nSet the height (i.e. number of rows) of this <code>DataFrame</code>.\nTry to reduce memory pressure at the expense of …\nSets the <code>Field</code> name.\nSet the null count directly.\nSerialize columns in parallel\nMake sure that all columns are contiguous in memory by …\nSet this <code>Series</code> as <code>sorted</code> so that downstream code can use …\nSet the ‘sorted’ bit meta info.\nChange the underlying <code>TimeUnit</code>. This does not modify the …\nChange the underlying <code>TimeUnit</code>. This does not modify the …\nChange the underlying <code>TimeUnit</code> and <code>TimeZone</code>. This does not …\nChange the underlying <code>TimeZone</code>. This does not modify the …\nSets the validity of this array.\nGet (height, width) of the <code>DataFrame</code>.\nShift the values by a given period and fill the parts that …\nShift every sub-array.\nShift the values by a given period and fill the parts that …\nShift every sublist.\nShift the values in the array by some period. See the …\nShift the values by a given period and fill the parts that …\nShift the values by a given period and fill the parts that …\nShift the values by a given period and fill the parts that …\nShift the values in the array by some period and fill the …\nReturns\nReturns true if the chunks of the columns do not align and …\nShrink numeric columns to the minimal required datatype …\nShrink the capacity of this array to fit its length.\nShrink the capacity of this array to fit its length.\nShrink the capacity of this array to fit its length.\nShrink the capacity of this DataFrame to fit its length.\nShrink the capacity of this array to fit its length.\nStream a query result into an csv file. This is useful if …\nStream a query result into an csv file in a partitioned …\nStream a query result into an ipc/arrow file. This is …\nStream a query result into an ipc/arrow file in a …\nStream a query result into a JSON file. This is useful if …\nStream a query result into a JSON file in a partitioned …\nStream a query result into a parquet file. This is useful …\nStream a query result into a parquet file in a partitioned …\nReturns the size as number of rows * number of columns\nGet the size of the binary values in bytes.\nSkip lines according to newline char (e.g. escaping will …\nSkip rows according to the CSV spec.\nGet a zero copy view of the data.\nSlice the DataFrame using an offset (starting row) and a …\nSlice every sublist.\nSlice the Series. <code>offset</code> may be negative.\nSlice the array. The chunks are reallocated the underlying …\nSlices this <code>StructArray</code>.\nSlice the <code>DataFrame</code> along the rows.\nTake only a slice of the result\nSlices this <code>Array</code>.\nSlices the <code>Array</code>.\nSlices this <code>StructArray</code>.\nReturns this array sliced.\nReturns this array sliced.\nReturned a sorted <code>ChunkedArray</code>.\nAdd a sort operation to the logical plan.\nSort every sublist.\nSort with given options.\nReturned a sorted <code>ChunkedArray</code>.\nReturn a sorted clone of this <code>DataFrame</code>.\nSort the series with specific options.\nSort this column by the ordering of another column …\nAdd a sort operation to the logical plan.\nSort <code>DataFrame</code> in place.\nGet the sources for this reader.\nGet a zero copy view of the data.\nSplit the array. The chunks are reallocated the underlying …\nSplit <code>DataFrame</code> at the given <code>offset</code>.\nCompute the square root of the given expression\nCheck if strings starts with a substring\nCompute and write column statistics.\nCompute the std of the items in every subarray.\nAggregate all the columns as their standard deviation …\nCompute the standard deviation of this ChunkedArray/Series.\nCompute the standard deviation of this ChunkedArray/Series.\nStandard deviation of the values of the Series.\nReturns the std value in the array Returns an option …\nReturns the std value in the array Returns an option …\nAggregate grouped <code>Series</code> and determine the standard …\nGet the standard deviation of the <code>ChunkedArray</code> as a new …\nGet the standard deviation of the Series as a new Series …\nGet the standard deviation of the Series as a new Series …\nGet the <code>string::StringNameSpace</code>\nUnpack to <code>ChunkedArray</code> of dtype <code>DataType::String</code>\nEscapes all regular expression meta characters in the …\nSlice the first <code>n</code> values of the string.\nGet the length of the string values as number of bytes.\nGet the length of the string values as number of chars.\nReverses the string values\nSlice the string values.\nSlice the last <code>n</code> values of the string.\nConvert from Time into String with the given format. See …\nConvert from Time into String with the given format. See …\nConvert from Time into String with the given format. See …\nConvert from Datetime into String with the given format. …\nConvert from Date into String with the given format. See …\nIf set then polars will return an error if any date …\nCast expression to another data type. Throws an error if …\nCast <code>AnyValue</code> to the provided data type and return a new …\nCast throws an error if conversion had overflows\nGets the number of strong (<code>Arc</code>) pointers to this …\nGet the <code>struct_::StructNameSpace</code>.\nUnpack to <code>ChunkedArray</code> of dtype <code>DataType::Struct</code>\nThe intersection of a source flags value with the …\nThe intersection of a source flags value with the …\nThe intersection of a source flags value with the …\nThe intersection of a source flags value with the …\nSubset of columns that will be taken into account.\nSuffix to add duplicate column names in join. Defaults to …\nAdd a suffix to the root column name.\nSum all the values in the column named <code>name</code>. Shorthand for …\nCompute the sum of the items in every subarray.\nAggregate all the columns as their sum values.\nCompute the sum the items in every sublist.\nAggregate the sum of the ChunkedArray. Returns <code>None</code> if not …\nAggregate the sum of the ChunkedArray. Returns <code>None</code> if not …\nReduce groups to the sum of all the values.\nAggregate grouped series and compute the sum per group.\nCompute the sum of all values in this Series. Returns …\nSum all values horizontally across columns.\nGet the sum of the <code>ChunkedArray</code> as a new <code>Series</code> of length …\nGet the sum of the Series as a new Scalar.\nGet the sum of the Series as a new Scalar.\nGet the sum of the Series as a new Series of length 1. …\nThe bitwise exclusive-or (<code>^</code>) of the bits in two flags …\nThe bitwise exclusive-or (<code>^</code>) of the bits in two flags …\nCall sync when closing the file.\nPerforms a set of actions within a synchronous update.\nGet the last <code>n</code> rows.\nReturn last n rows of each group\nGet the tail of every sublist\nGet the last <code>n</code> elements of the Expr result.\nGet the tail of the <code>ChunkedArray</code>\nGet the tail of the <code>DataFrame</code>.\nGet the tail of the Series.\nGather values from ChunkedArray by index.\nTake from <code>self</code> at the indexes given by <code>idx</code>.\nGather values from ChunkedArray by index.\nGather values from ChunkedArray by index.\nTake <code>DataFrame</code> rows by index values.\nGathers elements from a ChunkedArray, specifying for each …\nTake elements by a slice of <code>ChunkId</code>s.\nTake ownership of the underlying columns vec.\nSafety\nTake or clone a owned copy of the inner <code>ChunkedArray</code>.\nTake <code>Series</code> from a <code>Column</code>\nSafety\nTake elements by a slice of optional <code>ChunkId</code>s.\nTake function that checks of null state in <code>ChunkIdx</code>.\nTake from <code>self</code> at the indexes given by <code>idx</code>.\nTake from <code>self</code> at the indexes given by <code>idx</code>.\nSafety\nSafety\nSafety\nGather values from ChunkedArray by index.\nTake from <code>self</code> at the indexes given by <code>idx</code>.\nGather values from ChunkedArray by index.\nGather values from ChunkedArray by index.\nGather values from ChunkedArray by index.\nGather values from ChunkedArray by index.\nGather values from ChunkedArray by index.\nSafety\nSafety\nSafety\nTakes the validity of this array, leaving it without a …\nAdd a condition to the <code>when-then-otherwise</code> expression.\nUnpack to <code>ChunkedArray</code> of dtype <code>DataType::Time</code>\nUsed for <code>DataType::Time</code>.\nCreate a column of time ranges from a <code>start</code> and <code>stop</code> …\nConvert date(time) object to timestamp in <code>TimeUnit</code>.\nConvert date(time) object to timestamp in <code>TimeUnit</code>.\nConvert a List column into an Array column with the same …\nConvert to an Arrow data type.\nConverts the <code>Field</code> to an <code>arrow::datatypes::Field</code>.\nConvert a chunk in the Series to the correct Arrow type. …\nConvert to an Arrow Field\nConvert an <code>StringChunked</code> to a <code>Series</code> of <code>DataType::Decimal</code>. …\nGet a dot language representation of the LogicalPlan.\nGet a dot language representation of the streaming …\nCreate dummy variables.\nGet Field result of the expression. The schema is the …\nCast numerical types to f64, and keep floats as is.\nGet the name for <code>include_paths</code>\nCast the Array column to List column with the same inner …\nConvert a categorical column to its local representation.\nSafety\nSet the logical type of the <code>ListChunked</code>.\nReturns <code>&amp;self</code> for all but <code>ArrowDataType::Extension</code>. For …\nModify the strings to their lowercase equivalent.\nUpdate the root column name to use lowercase characters.\nTurn the scan source into a memory slice\nIf data is aligned in a single chunk and has no Null …\nIf all nested <code>Series</code> have the same length, a 2 dimensional …\nCreate a 2D <code>ndarray::Array</code> from this <code>DataFrame</code>. This …\nConvert to the physical data type\nConvert the datatype of the list into the physical …\nConvert the datatype of the array into the physical …\nConvert a struct to the underlying physical datatype.\nConverts a Series to their physical representation, if …\nMap to a physical type.\nthe <code>PhysicalType</code> of this <code>ArrowDataType</code>.\nConvert Time into String with the given format. See chrono …\nConvert Time into String with the given format. See chrono …\nConvert from <code>Duration</code> to String; note that <code>strftime</code> format …\nConvert from Datetime into String with the given format. …\nConvert from Date into String with the given format. See …\nConvert from Time into String with the given format. See …\nSafety\nModify the strings to their titlecase equivalent.\nModify the strings to their uppercase equivalent.\nUpdate the root column name to use uppercase characters.\nConvert to a <code>Vec</code> of <code>Option&lt;T::Native&gt;</code>.\nConvert to a <code>Vec</code> but don’t return <code>Option&lt;T::Native&gt;</code> if …\nThe bitwise exclusive-or (<code>^</code>) of the bits in two flags …\nThe bitwise exclusive-or (<code>^</code>) of the bits in two flags …\nA tolerance in the same unit as the asof column\nA time duration specified as a string, for example:\nTranspose a DataFrame. This is a very expensive operation.\nTrim all lists of unused start and end elements …\nTrim all lists of unused start and end elements …\nTrim all lists of unused start and end elements …\nTruncate the given ms timestamp by the window boundary.\nTruncate the given ns timestamp by the window boundary.\nTruncate the given us timestamp by the window boundary.\nAppend a new category, but fail if it didn’t exist yet …\nAppend a new category, but fail if it didn’t exist yet …\nApply a closure that may fail to a column. This is the …\nTry apply a closure <code>F</code> elementwise.\nTry apply a closure <code>F</code> to each array.\nApply a closure that may fail to a column at index <code>idx</code>. …\nApplies a function only to the non-null elements, …\nUnpack to <code>ChunkedArray</code> of dtype <code>DataType::Array</code>\nUnpack to <code>ChunkedArray</code> of dtype <code>DataType::Binary</code>\nUnpack to <code>ChunkedArray</code> of dtype <code>DataType::Binary</code>\nUnpack to <code>ChunkedArray</code> of dtype <code>DataType::Boolean</code>\nUnpack to <code>ChunkedArray</code> of dtype <code>DataType::Categorical</code>\nUnpack to <code>ChunkedArray</code> of dtype <code>DataType::Date</code>\nUnpack to <code>ChunkedArray</code> of dtype <code>DataType::Datetime</code>\nUnpack to <code>ChunkedArray</code> of dtype <code>DataType::Decimal</code>\nUnpack to <code>ChunkedArray</code> of dtype <code>DataType::Duration</code>\nUnpack to <code>ChunkedArray</code> of dtype <code>DataType::Float32</code>\nUnpack to <code>ChunkedArray</code> of dtype <code>DataType::Float64</code>\nCreate a new <code>DataFrame</code> from an iterator over rows. This …\nDeserializes [<code>crate::parquet::thrift_format::FileMetadata</code>] …\nGet column index of a <code>Series</code> by name.\nUnpack to <code>ChunkedArray</code> of dtype <code>DataType::Int128</code>\nUnpack to <code>ChunkedArray</code> of dtype <code>DataType::Int16</code>\nUnpack to <code>ChunkedArray</code>\nUnpack to <code>ChunkedArray</code> of dtype <code>DataType::Int64</code>\nUnpack to <code>ChunkedArray</code> of dtype <code>DataType::Int8</code>\nCreates a CSV reader using a file path.\nUnpack to <code>ChunkedArray</code> of dtype list\nMap a single dtype with a potentially failing mapper …\nMap all dtypes with a potentially failing mapper function.\nMap a single field with a potentially failing mapper …\nMap the dtype to the dtype of the array elements, with …\nConstructs a new <code>Arc&lt;T&gt;</code>, returning an error if allocation …\nReturns a new <code>StructArray</code>.\nConstructs a new <code>Arc&lt;T, A&gt;</code> in the provided allocator, …\nConstructs a new <code>Arc</code> with uninitialized contents, …\nConstructs a new <code>Arc</code> with uninitialized contents, in the …\nConstructs a new <code>Arc</code> with uninitialized contents, with the …\nConstructs a new <code>Arc</code> with uninitialized contents, with the …\nUnpack to <code>ChunkedArray</code> of dtype <code>DataType::Null</code>")